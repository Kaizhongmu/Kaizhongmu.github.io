[
  {
    "objectID": "ProjectsAndApps.html",
    "href": "ProjectsAndApps.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "ProjectsAndApps.html#projects-apps",
    "href": "ProjectsAndApps.html#projects-apps",
    "title": "",
    "section": "Projects & Apps",
    "text": "Projects & Apps\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\nLLM text classification Shiny App (R)\n\n\n\nFriday, August 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimulation Study Shiny App (R)\n\n\n\nSaturday, March 1, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/sp25.html",
    "href": "teaching/sp25.html",
    "title": "Spring 2025: MAC2311 Calculus With Analytic Geometry I",
    "section": "",
    "text": "COURSE MEETING SCHEDULE:\nMondays, Wednesdays, and Fridays\nTime: 10:40 AM - 11:30 AM Location: HTL 215 Thursdays\nTime: 11:35 AM - 12:50 PM Location: HTL 215\nCREDIT HOURS: 4\nTime Zone: This course is a Main Campus FSU course and all times provided for this course (class meetings, due dates, etc) will be in Eastern Standard Time.\nCOURSE INSTRUCTOR\nRAFIQ ISLAM\nrislam@fsu.edu\nOffice Hours: Monday 12:00-1:00, Tuesday 11:30-12:30, Wednesday  12:00 - 1:00  Office: Lov 331\nCOURSE DESCRIPTION\nIn this course, students will develop problem solving skills, critical thinking, computational proficiency, and contextual fluency through the study of limits, derivatives, and definite and indefinite integrals of functions of one variable, including algebraic, exponential, logarithmic, and trigonometric functions, and applications. Topics will include limits, continuity, differentiation and rates of change, optimization, curve sketching, and introduction to integration and area.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "teaching/sp23.html",
    "href": "teaching/sp23.html",
    "title": "Spring 2023: MAC1140 PreCalculus Algebra",
    "section": "",
    "text": "PreCalculus and Algebra are one of the many important foundation math courses that open doors to many upper-level math and science courses. The topic of this course includes but not is limited to Complex Numbers, Piecewise Functions, Quadratic Functions, Polynomial Functions, Polynomial Division, Zeros of Polynomials, Rational Functions, Polynomial and Rational Inequalities, Inverse Functions, Exponential Functions, Logarithmic Functions, Properties of Logarithms, Exponential and Logarithmic Equations, and so on.  As an Instructor of Record for this course, I taught a class of 27 undergraduate students from different majors. I also proctor their lab classes where they take their quizzes and tests online and other application based lab activities.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "teaching/f18-f21.html",
    "href": "teaching/f18-f21.html",
    "title": "Fall 2018 to Spring 2020: College Algebra, Trigonometry",
    "section": "",
    "text": "Responsible for the preparation and delivery of all lectures, making question paper for all exams, and the grading of tests and homework assignments for the following courses: - College Algebra, Fall 2018 - Trigonometry, Fall 2019 and Spring 2020\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "portfolio/spd/desgld/index.html",
    "href": "portfolio/spd/desgld/index.html",
    "title": "Python Application Library: desgld packaging",
    "section": "",
    "text": "This package is related to my ongoing project “EXTRA decentralized stochastic gradient Langevin dynamics”. The detail of the package can be found here\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{islam2024,\n  author = {Islam, Rafiq},\n  title = {Python {Application} {Library:} Desgld Packaging},\n  date = {2024-05-03},\n  url = {https://kaizhongmu.github.io/portfolio/spd/desgld/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nIslam, Rafiq. 2024. “Python Application Library: Desgld\nPackaging.” May 3, 2024. https://kaizhongmu.github.io/portfolio/spd/desgld/."
  },
  {
    "objectID": "portfolio/spd/webapp/index.html",
    "href": "portfolio/spd/webapp/index.html",
    "title": "Streamlit Web App",
    "section": "",
    "text": "Welcome to MiCharge Predictor! This web app is a part of my data science project Insurance Cost Forecast by using Linear Regression, aimed at predicting the medical cost based on various personal and lifestyle factors. By leveraging advanced machine learning techniques, MiCharge Predictor provides an approximate estimates to help users understand potential medical expances.\n\n\n\n\nUser-Friendly Interface: Easily input your personal details and receive instant predictions.\n\nCopmprehensive Data Analysis: Utilizes sophisticated algorithms to analyze factors such as age, BMI, smoking habits, and more.\n\nAccessibility: Available both on web and mobile platforms.\n\n\n\n\n\nInput Your Data: Enter details such as age, gender, BMI, number of children, smoking status, and region.\n\nage: Minimum 18, maximum 100\ngender: Male or Female\nBMI: Minimum 15.0, maximum 60\nnumber of children: Takes values from 0 to 5\nsmoking: Yes or No\nregion: Takes four string input: Northeast, Northwest, Southeast, Southwest\n\nAnalyze: The algorithm process the given input\nGet Prediction: Based on the input, you get the output."
  },
  {
    "objectID": "portfolio/spd/webapp/index.html#about",
    "href": "portfolio/spd/webapp/index.html#about",
    "title": "Streamlit Web App",
    "section": "",
    "text": "Welcome to MiCharge Predictor! This web app is a part of my data science project Insurance Cost Forecast by using Linear Regression, aimed at predicting the medical cost based on various personal and lifestyle factors. By leveraging advanced machine learning techniques, MiCharge Predictor provides an approximate estimates to help users understand potential medical expances.\n\n\n\n\nUser-Friendly Interface: Easily input your personal details and receive instant predictions.\n\nCopmprehensive Data Analysis: Utilizes sophisticated algorithms to analyze factors such as age, BMI, smoking habits, and more.\n\nAccessibility: Available both on web and mobile platforms.\n\n\n\n\n\nInput Your Data: Enter details such as age, gender, BMI, number of children, smoking status, and region.\n\nage: Minimum 18, maximum 100\ngender: Male or Female\nBMI: Minimum 15.0, maximum 60\nnumber of children: Takes values from 0 to 5\nsmoking: Yes or No\nregion: Takes four string input: Northeast, Northwest, Southeast, Southwest\n\nAnalyze: The algorithm process the given input\nGet Prediction: Based on the input, you get the output."
  },
  {
    "objectID": "portfolio/spd/webapp/index.html#usage",
    "href": "portfolio/spd/webapp/index.html#usage",
    "title": "Streamlit Web App",
    "section": "Usage",
    "text": "Usage\nYou can use the app directly from streamlit web using this link or just here.."
  },
  {
    "objectID": "portfolio/dsp/lendingclub/index.html",
    "href": "portfolio/dsp/lendingclub/index.html",
    "title": "Lendingclub’s loan default prediction",
    "section": "",
    "text": "Notebook"
  },
  {
    "objectID": "portfolio/dsp/lendingclub/index.html#project-overview",
    "href": "portfolio/dsp/lendingclub/index.html#project-overview",
    "title": "Lendingclub’s loan default prediction",
    "section": "Project Overview",
    "text": "Project Overview\n\nLendingClub is a U.S.-based financial services company that initially began as a peer-to-peer (P2P) lending platform, allowing individual investors to lend directly to individual borrowers through a marketplace. Founded in 2006, it grew quickly to become one of the largest and most popular P2P lending platforms, helping connect borrowers in need of personal loans with investors looking for alternative investment opportunities.  LendingClub has been known for its transparent data-sharing practices, making anonymized loan data available to researchers, data scientists, and investors. This data is widely used in financial research, especially for predictive modeling of loan risk and borrower behavior.   The aim of this data science project is to build a machine learning model to predict the likelihood of a loan default.\n\n\nMore information about LendingClub\n\nP2P Lending Model (Early Focus):\n\nBorrowers could apply for loans, typically unsecured personal loans, through LendingClub’s platform.\nLoans were then funded by individual investors who could review borrowers’ profiles, risk grades, and other financial information before committing funds.\nThe model offered borrowers a way to access loans outside traditional banks, often at lower interest rates, and allowed investors to diversify by spreading investments across multiple loans.\n\nRisk and Return:\n\nLendingClub assigned credit grades (A–G) to each loan based on creditworthiness, which affected interest rates. Higher risk meant potentially higher returns for investors but also higher chances of default.\nInvestors bore the risk if a borrower defaulted, which was a notable risk factor compared to FDIC-insured deposits.\n\nShift to a Bank Model:\n\nOver time, LendingClub transitioned away from P2P lending and restructured as a more traditional bank, obtaining a bank charter in 2021.\nIt now offers banking products, such as high-yield savings accounts, and operates more like a digital bank while still focusing on lending products.\n\nBorrower and Loan Profiles:\n\nLendingClub primarily focuses on personal loans for debt consolidation, credit card refinancing, home improvement, and other purposes.\nBorrowers’ profiles typically include information on income, credit score, debt-to-income ratio, and loan purpose, which is used for assessing risk."
  },
  {
    "objectID": "portfolio/dsp/lendingclub/index.html#dataset",
    "href": "portfolio/dsp/lendingclub/index.html#dataset",
    "title": "Lendingclub’s loan default prediction",
    "section": "Dataset",
    "text": "Dataset\n\nThe dataset is a publicly available data from kaggle.com. It originally contains 396030 entries, with 100.4 MB. However, for easy github push, I reduce the dataset slightly in order to have size smaller than 100 MB. So, in the reduced form, it has 395900 entries with the following columns:\n\n\nloan_amnt: The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.\n\nterm: The number of payments on the loan. Values are in months and can be either 36 or 60.\n\nint_rate: Interest Rate on the loan installment The monthly payment owed by the borrower if the loan originates.\n\ngrade: LC assigned loan grade\n\nsub_grade: LC assigned loan subgrade\n\nemp_title: The job title supplied by the Borrower when applying for the loan.\n\nemp_length: Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years.\n\nhome_ownership: The home ownership status provided by the borrower during registration or obtained from the credit report. Our values are: RENT, OWN, MORTGAGE, OTHER\n\nannual_inc: The self-reported annual income provided by the borrower during registration.\n\nverification_status: Indicates if income was verified by LC, not verified, or if the income source was verified\n\nissue_d: The month which the loan was funded\n\nloan_status: Current status of the loan\n\npurpose: A category provided by the borrower for the loan request.\n\ntitle: The loan title provided by the borrower\n\nzip_code: The first 3 numbers of the zip code provided by the borrower in the loan application.\n\naddr_state: The state provided by the borrower in the loan application.\n\ndti: A ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income.\n\nearliest_cr_line: The month the borrower’s earliest reported credit line was opened.\n\nopen_acc: The number of open credit lines in the borrower’s credit file.\n\npub_rec: Number of derogatory public records.\n\nrevol_bal: Total credit revolving balance.\n\nrevol_util: Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.\n\ntotal_acc: The total number of credit lines currently in the borrower’s credit file\ninitial_list_status: The initial listing status of the loan. Possible values are – W, F\napplication_type: Indicates whether the loan is an individual application or a joint application with two co-borrowers.\n\nmort_acc: Number of mortgage accounts.\n\npub_rec_bankruptcies: Number of public record bankruptcies\n\n\nHere loan_status is the predictive or dependent variable and the rest are predicting or independent variables aka features. We want to predict if the loan will be Fully Paid or Charged Off given the feature values."
  },
  {
    "objectID": "portfolio/dsp/lendingclub/index.html#stakeholders",
    "href": "portfolio/dsp/lendingclub/index.html#stakeholders",
    "title": "Lendingclub’s loan default prediction",
    "section": "Stakeholders",
    "text": "Stakeholders"
  },
  {
    "objectID": "portfolio/dsp/lendingclub/index.html#key-performance-indicators-kpis-of-lendingclub",
    "href": "portfolio/dsp/lendingclub/index.html#key-performance-indicators-kpis-of-lendingclub",
    "title": "Lendingclub’s loan default prediction",
    "section": "Key Performance Indicators (KPIs) of LendingClub",
    "text": "Key Performance Indicators (KPIs) of LendingClub"
  },
  {
    "objectID": "portfolio/dsp/lendingclub/index.html#modeling",
    "href": "portfolio/dsp/lendingclub/index.html#modeling",
    "title": "Lendingclub’s loan default prediction",
    "section": "Modeling",
    "text": "Modeling"
  },
  {
    "objectID": "portfolio/dsp/lendingclub/index.html#results-and-outcome",
    "href": "portfolio/dsp/lendingclub/index.html#results-and-outcome",
    "title": "Lendingclub’s loan default prediction",
    "section": "Results and Outcome",
    "text": "Results and Outcome"
  },
  {
    "objectID": "portfolio/dsp/lendingclub/index.html#future-directions",
    "href": "portfolio/dsp/lendingclub/index.html#future-directions",
    "title": "Lendingclub’s loan default prediction",
    "section": "Future Directions",
    "text": "Future Directions"
  },
  {
    "objectID": "portfolio/dsp/titanic/index.html",
    "href": "portfolio/dsp/titanic/index.html",
    "title": "Classification Probelm: Predict the chance of survival of a voager on Titanic based on the voager’s information",
    "section": "",
    "text": "Notebook\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{islam2021,\n  author = {Islam, Rafiq},\n  title = {Classification {Probelm:} {Predict} the Chance of Survival of\n    a Voager on {Titanic} Based on the Voager’s Information},\n  date = {2021-10-15},\n  url = {https://kaizhongmu.github.io/portfolio/dsp/titanic/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nIslam, Rafiq. 2021. “Classification Probelm: Predict the Chance of\nSurvival of a Voager on Titanic Based on the Voager’s\nInformation.” October 15, 2021. https://kaizhongmu.github.io/portfolio/dsp/titanic/."
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html",
    "href": "portfolio/dsp/medicalcost/index.html",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "",
    "text": "Notebook GitHub WebApp"
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html#project-overview",
    "href": "portfolio/dsp/medicalcost/index.html#project-overview",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Project Overview",
    "text": "Project Overview\n\nThis predictive modeling project involves personal medical data to predict the medical insurance charge by using a linear regression model."
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html#dataset",
    "href": "portfolio/dsp/medicalcost/index.html#dataset",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Dataset",
    "text": "Dataset\nThe dataset used in this project is collected from Kaggle\nColumns\nage: age of primary beneficiary\nsex: insurance contractor gender, female, male\nbmi: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight \\(\\frac{kg}{m^ 2}\\) using the ratio of height to weight, ideally \\(18.5\\) to \\(24.9\\)\nchildren: Number of children covered by health insurance / Number of dependents\nsmoker: Smoking\nregion: the beneficiary’s residential area in the US, northeast, southeast, southwest, northwest.\ncharges: Individual medical costs billed by health insurance\nAcknowledgements\nThe dataset is available on GitHub here."
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html#stakeholders",
    "href": "portfolio/dsp/medicalcost/index.html#stakeholders",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Stakeholders",
    "text": "Stakeholders\nCan we accurately predict insurance costs?"
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html#key-performance-indicators-kpis",
    "href": "portfolio/dsp/medicalcost/index.html#key-performance-indicators-kpis",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Key Performance Indicators (KPIs)",
    "text": "Key Performance Indicators (KPIs)\n\nAll the features were considered for the modeling purposes. However, from the exploratory data analysis and mathematical analysis, it was found that the charges usually goes up for the factors such as increase in age, living in certain region, having certain number of children. But this is not always the same depending on the smoker variable. Also, there is a strong correlation between age and bmi variable. Age a result new features such as age_bmi and age_bmi_smoker features were created to see how the charges interact."
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html#modeling",
    "href": "portfolio/dsp/medicalcost/index.html#modeling",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Modeling",
    "text": "Modeling\n\nModeling Approaches\nWe consider the following models\n\nBaseline model: Assumption that the charges variable can be modeled with the mean value of this charges variable.\n\\[\n\\text{charges}=\\mathbb{E}[\\text{charges}]+\\xi\n\\]\nLinear Regression with age-bmi-smoke interaction\n\\[\n\\text{charges}=\\beta_0+\\beta_1 (\\text{age\\_bmi})+\\beta_2 (\\text{male})+\\beta_3 (\\text{smoke})+\\beta_4 (\\text{children})+\\beta_5 (\\text{region})+\\beta_6 (\\text{age-bmi-smoke})+\\xi\n\\]\nK-Neighbor Regression\n\\(k\\)NN using all the original feature with \\(k=10\\)\n\n\n\nFinal Model\nFinally the modeling was done based on the lowest MSE value found from the 5-fold cross validation and the model has the following form\n\\[\\begin{align*}\n\\text{charges} &=10621.25+ 3346.14\\times \\text{Age\\_BMI}+4570.76\\times \\text{Male}+ 479.61\\times \\text{Smoke}-315.12\\times \\text{Children}\\\\\n&+13274.48\\times \\text{Region}-212.22\\times \\text{Age\\_BMI\\_Smoke}\n\\end{align*}\\]"
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html#results-and-outcomes",
    "href": "portfolio/dsp/medicalcost/index.html#results-and-outcomes",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Results and Outcomes",
    "text": "Results and Outcomes\n\nModel Accuracy\nThe model above returns an RMSE of \\(5853.0\\) on the training set and an RMSE of \\(5600.0\\) on the test set with an \\(R^2=80\\%\\).\n\n\nWeb Application\nThe final model was developed and deployed using Streamlit. To try a single instance, fill out the following form and then click predict charges."
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html#future-directions",
    "href": "portfolio/dsp/medicalcost/index.html#future-directions",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Future Directions",
    "text": "Future Directions\nFuture project on the same data could be adding a neural network and compare the relative performances of the two models.\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet"
  },
  {
    "objectID": "Life.html",
    "href": "Life.html",
    "title": "Life",
    "section": "",
    "text": "After graduating with my bachelor’s degree, I embarked on a solo journey across the United States from the West Coast to the East Coast. I traversed 19 states, winding through the Sierra Nevada mountains, rolling past Texas oil fields, and enjoying sun-kissed beaches of Florida. On one remarkable day, I experienced the changing of all four seasons.\n\n\n\nI love sailing—thriving in the exhilaration of sudden storms at sea, yet equally cherishing the quiet moments on calm waters, reflecting on life. And sailing is a team sport, built on trust and coordination, where every crew member matters—this is also why I like it.\n\n\n\nI once cycled with my friend all the way from Tianjin to Beijing, China, embracing the challenge and freedom of the journey. Believing that cycling should not be limited to those who can afford expensive equipment, I co-founded the Tianjin United Cycling Club (UCST) in high school, bringing together students from 13 schools to share and experience the joy of cycling."
  },
  {
    "objectID": "Life.html#life",
    "href": "Life.html#life",
    "title": "Life",
    "section": "",
    "text": "After graduating with my bachelor’s degree, I embarked on a solo journey across the United States from the West Coast to the East Coast. I traversed 19 states, winding through the Sierra Nevada mountains, rolling past Texas oil fields, and enjoying sun-kissed beaches of Florida. On one remarkable day, I experienced the changing of all four seasons.\n\n\n\nI love sailing—thriving in the exhilaration of sudden storms at sea, yet equally cherishing the quiet moments on calm waters, reflecting on life. And sailing is a team sport, built on trust and coordination, where every crew member matters—this is also why I like it.\n\n\n\nI once cycled with my friend all the way from Tianjin to Beijing, China, embracing the challenge and freedom of the journey. Believing that cycling should not be limited to those who can afford expensive equipment, I co-founded the Tianjin United Cycling Club (UCST) in high school, bringing together students from 13 schools to share and experience the joy of cycling."
  },
  {
    "objectID": "research/graph_theory/index.html",
    "href": "research/graph_theory/index.html",
    "title": "Neuroimaging & Network Data",
    "section": "",
    "text": "Graph Theory, fMRI, PET, Machine Learning, High-dimensional"
  },
  {
    "objectID": "research/graph_theory/index.html#interest-keywords",
    "href": "research/graph_theory/index.html#interest-keywords",
    "title": "Neuroimaging & Network Data",
    "section": "",
    "text": "Graph Theory, fMRI, PET, Machine Learning, High-dimensional"
  },
  {
    "objectID": "research/graph_theory/index.html#related-experience",
    "href": "research/graph_theory/index.html#related-experience",
    "title": "Neuroimaging & Network Data",
    "section": "Related Experience",
    "text": "Related Experience\n\nResearch Project: Non-Parametric Graph-Based Change-Point Detection Method for High-Dimensional Network Data\nResearch Project: An Bayesian-based RNN Method for MNAR/MAR + left-censored longitudinal\nCollaboration: Translational Medical Research collaboration"
  },
  {
    "objectID": "research/graph_theory/index.html#related-story",
    "href": "research/graph_theory/index.html#related-story",
    "title": "Neuroimaging & Network Data",
    "section": "Related Story",
    "text": "Related Story\nI am deeply grateful to Dr. Hao Chen for introducing me to graph theory during my first research project, where it was applied to detect change points in social network data. This experience opened doors to numerous possibilities like high-dimensional data, omic data, neuroimaging. These are all areas I hope to explore further if the opportunity arises."
  },
  {
    "objectID": "research/ai/index.html",
    "href": "research/ai/index.html",
    "title": "AI + Statistics",
    "section": "",
    "text": "Bayesian Neural Networks, Interpretable AI, LLM, Statistical Machine Learning"
  },
  {
    "objectID": "research/ai/index.html#interest-keywords",
    "href": "research/ai/index.html#interest-keywords",
    "title": "AI + Statistics",
    "section": "",
    "text": "Bayesian Neural Networks, Interpretable AI, LLM, Statistical Machine Learning"
  },
  {
    "objectID": "research/ai/index.html#related-experience",
    "href": "research/ai/index.html#related-experience",
    "title": "AI + Statistics",
    "section": "Related Experience",
    "text": "Related Experience\n\nHealth Data Science Fellowship: Applying LLM to free text classification in clinical text data\nResearch Project: An Bayesian-based RNN Method for MNAR/MAR + left-censored longitudinal\nResearch Project: Graph-Based Change-Point Detection Method for High-Dimensional Network Data\nWorkshop: AI workshop NYC 2025\nBlog: How AI (CNN) threatens the doctors?"
  },
  {
    "objectID": "research/ai/index.html#related-story",
    "href": "research/ai/index.html#related-story",
    "title": "AI + Statistics",
    "section": "Related Story",
    "text": "Related Story\n\n2020: Majored in statistics; began 50% Stats + 50% DS training at UC Davis.\n2021: Completed 1st DS project with Mingyu Zhu, a friendship started from it\n2022: Involved first ML+Stats research project mentored by Dr. Hao Chen\n2023: Conducted first independent research about ML mentored by Dr. Colin Cameron\n2024: Inward sharpening\n2025: Led Bayesian-based RNN method development mentored by Dr. Ani Eloyan."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "publication/pub1/index.html",
    "href": "publication/pub1/index.html",
    "title": "Comparison of financial models for stock price prediction",
    "section": "",
    "text": "Time series analysis of daily stock data and building predictive models are complicated. This project presents a comparative study for stock price prediction using three different methods, namely autoregressive integrated moving average, artificial neural network, and stochastic process-geometric Brownian motion. Each of the methods is used to build predictive models using historical stock data collected from Yahoo Finance. Finally, output from each of the models is compared to the actual stock price. Empirical results show that the conventional statistical model and the stochastic model provide better approximation for next-day stock price prediction compared to the neural network model.\n\n\n    \n        \n    \n    \n        \n    \n\n\nShare on\n\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@article{rafiqul_islam2020,\n  author = {Rafiqul Islam, Mohammad and Nguyen, Nguyet},\n  publisher = {MDPI},\n  title = {Comparison of Financial Models for Stock Price Prediction},\n  journal = {Journal of Risk and Financial Management},\n  date = {2020-08-14},\n  url = {https://www.mdpi.com/1911-8074/13/8/181},\n  doi = {10.3390/jrfm13080181},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRafiqul Islam, Mohammad, and Nguyet Nguyen. 2020. “Comparison of\nFinancial Models for Stock Price Prediction.” Journal of Risk\nand Financial Management, August. https://doi.org/10.3390/jrfm13080181."
  },
  {
    "objectID": "project/dsp/1/index.html",
    "href": "project/dsp/1/index.html",
    "title": "Simulation Study Shiny App (R)",
    "section": "",
    "text": "&lt;p&gt;\nYour browser does not support iframes. &lt;a href=\"https://brown-biostatistics-mikemu.shinyapps.io/SRB_simulation/\" target=\"_blank\"&gt;Open SRB Simulation App&lt;/a&gt;\n&lt;/p&gt;\n\nSRB Simulation Study: This Shiny App is designed to show the simulation study results for my second research project under the mentorship of Dr. Ani Eloyan. Our simulation involves multiple parameters (e.g., rate of decline, baseline severity, sample size). Static plots can only capture a few pre-selected scenarios, failing to show the full picture. So, I developed an interactive Shiny app to explore any combination of parameters in real-time.\nProject Details: The detail could check in A Bayesian Cognitive Change Method for Censored Longitudinal Data.\nDirect Link to App"
  },
  {
    "objectID": "project/dsp/1/index.html#shiny-app",
    "href": "project/dsp/1/index.html#shiny-app",
    "title": "Simulation Study Shiny App (R)",
    "section": "",
    "text": "&lt;p&gt;\nYour browser does not support iframes. &lt;a href=\"https://brown-biostatistics-mikemu.shinyapps.io/SRB_simulation/\" target=\"_blank\"&gt;Open SRB Simulation App&lt;/a&gt;\n&lt;/p&gt;\n\nSRB Simulation Study: This Shiny App is designed to show the simulation study results for my second research project under the mentorship of Dr. Ani Eloyan. Our simulation involves multiple parameters (e.g., rate of decline, baseline severity, sample size). Static plots can only capture a few pre-selected scenarios, failing to show the full picture. So, I developed an interactive Shiny app to explore any combination of parameters in real-time.\nProject Details: The detail could check in A Bayesian Cognitive Change Method for Censored Longitudinal Data.\nDirect Link to App"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "&lt;p&gt;Your browser does not support PDFs. \n               &lt;a href=\"/_assets/documents/CV.pdf\" target=\"_blank\"&gt;Download the PDF&lt;/a&gt;\n            &lt;/p&gt;\n        \n    \n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/bengalitrial/index.html",
    "href": "posts/bengalitrial/index.html",
    "title": "বাংলা ভাষায় আমার লেখা || My Blog in Benglali Language",
    "section": "",
    "text": "“বাংলায় ব্লগিং করতে পারলে ভালই হতো” এমন ভাবনা থেকেই ঘাটাঘটি শুরু করলাম কিভাবে নিজের ব্লগে বাংলায় লিখতে পারি। বাংলায় মনের ভাব প্রকাশের অসংখ্য মাধ্যম রয়েছে। সামাজিক যোগাযোগের মাধ্যম, পত্রিকা, কিংবা অন্যান্য প্রতিষ্ঠিত ব্লগ। কিন্তু নিজের ব্লগে নিজে বাংলায় লিখতে পারবো কিনা তা নিয়ে একটু সংশয় ছিল কারিগরি দিকটা নিয়ে। কোয়ার্তো দিয়ে আমার এই ব্লগ সাইট বানানো। তাই কোয়ার্তোর ওয়েবসাইট ঘাঁটতে ঘাঁটতে আজ পেয়ে গেলাম কিভাবে ইউনিকোড দিয়ে লিখা যায়। এখন থেকে মাঝে মধ্যেই এখানে বাংলায় পোষ্ট করবো। দেখা যাক।"
  },
  {
    "objectID": "posts/bengalitrial/index.html#আপনক-সবগতম",
    "href": "posts/bengalitrial/index.html#আপনক-সবগতম",
    "title": "বাংলা ভাষায় আমার লেখা || My Blog in Benglali Language",
    "section": "",
    "text": "“বাংলায় ব্লগিং করতে পারলে ভালই হতো” এমন ভাবনা থেকেই ঘাটাঘটি শুরু করলাম কিভাবে নিজের ব্লগে বাংলায় লিখতে পারি। বাংলায় মনের ভাব প্রকাশের অসংখ্য মাধ্যম রয়েছে। সামাজিক যোগাযোগের মাধ্যম, পত্রিকা, কিংবা অন্যান্য প্রতিষ্ঠিত ব্লগ। কিন্তু নিজের ব্লগে নিজে বাংলায় লিখতে পারবো কিনা তা নিয়ে একটু সংশয় ছিল কারিগরি দিকটা নিয়ে। কোয়ার্তো দিয়ে আমার এই ব্লগ সাইট বানানো। তাই কোয়ার্তোর ওয়েবসাইট ঘাঁটতে ঘাঁটতে আজ পেয়ে গেলাম কিভাবে ইউনিকোড দিয়ে লিখা যায়। এখন থেকে মাঝে মধ্যেই এখানে বাংলায় পোষ্ট করবো। দেখা যাক।"
  },
  {
    "objectID": "posts/matrixrep/index.html",
    "href": "posts/matrixrep/index.html",
    "title": "Matrix Representation: Change of Basis",
    "section": "",
    "text": "Let \\(\\alpha: \\mathcal{P}_2(\\mathbb{R}) \\longrightarrow M_{2\\times 2}(\\mathbb{R})\\) be defined by\n\n\n\\(\\alpha(f(x))=\\left(\\begin{array}{cc}f^{'}(0)& 2f(1)\\\\0& f^{''}(3)\\end{array}\\right)\\)\n\n\nFirst, let’s show that \\(\\alpha\\) is a linear transformation. Let \\(f(x),g(x) \\in \\mathcal{P}_2(\\mathbb{R})\\) and \\(a,b\\in \\mathbb{R}\\). Then by definition, we have\n\n\n\\(\\alpha(af(x)+bg(x))=\\left(\\begin{array}{cc}af'(0)+bg'(0)& 2af(1)+2bg(1)\\\\0& af''(3)+bg''(3)\\end{array}\\right)\\)\n\n\n\n\n\n\\(\\hspace{1.6in}\\)=\\(\\left(\\begin{array}{cc}af'(0)& 2af(1)\\\\0& af''(3)\\end{array}\\right)\\)+\\(\\left(\\begin{array}{cc}bg'(0)& 2bg(1)\\\\0& bg''(3)\\end{array}\\right)\\)\n\n\n\n\n\n\\(\\hspace{1.6in}\\)=\\(a\\left(\\begin{array}{cc}f'(0)& 2f(1)\\\\0& f''(3)\\end{array}\\right)\\)+\\(b\\left(\\begin{array}{cc}g'(0)& 2g(1)\\\\0& g''(3)\\end{array}\\right)\\)\n\n\n\n\n\n\\(\\hspace{1.6in}=a\\alpha(f(x))+b\\alpha(g(x))\\)\n\n\nSo that \\(\\alpha\\) is a linear transformation.\n\n\nSecond, we find the kernel space \\(ker(\\alpha)\\), then use the Dimension Theorem (formula) to decide the rank of \\(\\alpha\\)\n\n\nThe kernel of \\(\\alpha\\) is defined as\n\n\n\\(ker(\\alpha)=\\{v\\in V|\\alpha(v)=0_{M_{2\\times2}(\\mathbb{R})}\\}\\)\n\n\n\n\n\n\\(\\alpha(f(x))=\\left(\\begin{array}{cc}f^{'}(0)& 2f(1)\\\\0& f^{''}(3)\\end{array}\\right)=[0]\\)\n\n\n\\(\\implies f'(0)=0, 2f(1)=0, f''(3)=0\\)\n\n\nIf \\(f(x)=a+bx+cx^2\\) then we have,\n\n\n\\(\\begin{array}{c}f'(0)\\implies b=0\\\\2f(1)=0\\implies 2(a+b+c)=0\\\\f''(3)=0\\implies 2c=0\\end{array}\\)\n\n\n\\(\\implies a=b=c=0 \\implies ker(\\alpha)=\\{0_{\\mathcal{P}_2(\\mathbb{R})}\\}\\)\n\n\nThen \\(nullity(\\alpha)=\\dim ker(\\alpha)=0\\) and if we use the dimension formula then, \\(rank(\\alpha)=\\dim \\mathcal{P}_2(\\mathbb{R})-nullity(\\alpha)=3-0=3\\)\n\n\nThird, we will find the representation matrix \\(\\phi_{BD}(\\alpha)\\), where \\(B=\\{1+x,1-x,x^2\\}\\) is an ordered basis for \\(\\mathcal{P}_2(\\mathbb{R})\\)\n\n\nand\n\n\\(D=\\begin{Bmatrix}\\begin{bmatrix}1 & 0\\\\0 &0\\end{bmatrix},\\begin{bmatrix}0 & 1\\\\0 &0\\end{bmatrix},\\begin{bmatrix}0 & 0\\\\1 &0\\end{bmatrix},\\begin{bmatrix}0 & 0\\\\0 &1\\end{bmatrix}\\end{Bmatrix}\\)\nis an ordered basis for \\(\\mathbf{M}_{2\\times 2}(\\mathbb{R})\\)\n\n\n\nNow if \\(f(x)=1+x\\) then\n\n\n\\(\\alpha(f(x))=\\left(\\begin{array}{cc}f^{'}(0)& 2f(1)\\\\0& f^{''}(3)\\end{array}\\right)\\)\n\n\n\n\n\n\\(\\hspace{1in}=\\left(\\begin{array}{cc}1& 4\\\\0& 0\\end{array}\\right)\\)\n\n\n\n\n\n\\(\\hspace{1in}=\\left(\\begin{array}{c}1\\\\4\\\\0\\\\0\\end{array}\\right)\\)\n\n\n\n\n\n\nNow if \\(f(x)=1-x\\) then\n\n\n\\(\\alpha(f(x))=\\left(\\begin{array}{cc}f^{'}(0)& 2f(1)\\\\0& f^{''}(3)\\end{array}\\right)\\)\n\n\n\n\n\n\\(\\hspace{1in}=\\left(\\begin{array}{cc}-1& 0\\\\0& 0\\end{array}\\right)\\)\n\n\n\n\n\n\\(\\hspace{1in}=\\left(\\begin{array}{c}-1\\\\0\\\\0\\\\0\\end{array}\\right)\\)\n\n\n\n\n\n\nNow if \\(f(x)=x^2\\) then\n\n\n\\(\\alpha(f(x))=\\left(\\begin{array}{cc}f^{'}(0)& 2f(1)\\\\0& f^{''}(3)\\end{array}\\right)\\)\n\n\n\n\n\n\\(\\hspace{1in}=\\left(\\begin{array}{cc}0& 2\\\\0& 2\\end{array}\\right)\\)\n\n\n\n\n\n\\(\\hspace{1in}=\\left(\\begin{array}{c}-2\\\\2\\\\0\\\\2\\end{array}\\right)\\)\n\n\n\n\n\nbecause,\n\n\n\n\n\n\n\\(\\left(\\begin{array}{cc}0& 2\\\\0& 2\\end{array}\\right)\\)\\(=-2\\left(\\begin{array}{cc}1& 0\\\\0& 0\\end{array}\\right)\\)\\(+2\\left(\\begin{array}{cc}0& 1\\\\0& 0\\end{array}\\right)\\)\\(+0\\left(\\begin{array}{cc}0& 0\\\\1& 0\\end{array}\\right)\\)\\(+2\\left(\\begin{array}{cc}1& 0\\\\0& 1\\end{array}\\right)\\)\n\n\n\n\n\nTherefore, \\(\\phi_{BD}(\\alpha)=\\)\\(\\left(\\begin{array}{ccc}1& -1& -2\\\\4& 0& 2\\\\0& 0& 0\\\\0& 0& 2\\end{array}\\right)\\)\n\n\n\n\n\n\n\n\n\n\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized eigenvectors and eigenspaces\n\n2 min\n\n\nRafiq Islam\n\n\nMonday, January 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementation of Gradient Descent (GD) and Stochastic Gradient Descent (SGD)\n\n14 min\n\n\nRafiq Islam\n\n\nThursday, September 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nLU Factorization of a Full rank Matrix using Fortran\n\n26 min\n\n\nRafiq Islam\n\n\nTuesday, November 9, 2021\n\n\n\n\n\n\nNo matching items\n Back to topCitationBibTeX citation:@online{islam2021,\n  author = {Islam, Rafiq},\n  title = {Matrix {Representation:} {Change} of {Basis}},\n  date = {2021-01-21},\n  url = {https://kaizhongmu.github.io/posts/matrixrep/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nIslam, Rafiq. 2021. “Matrix Representation: Change of\nBasis.” January 21, 2021. https://kaizhongmu.github.io/posts/matrixrep/."
  },
  {
    "objectID": "posts/eigen/index.html",
    "href": "posts/eigen/index.html",
    "title": "Generalized eigenvectors and eigenspaces",
    "section": "",
    "text": "Definition: Let \\(\\alpha\\in End(V)\\) and \\(\\lambda\\in spec(\\alpha)\\). A non-zero vector \\(v\\) is called a generalized eigenvector vector of \\(\\alpha\\) associated with \\(\\lambda\\) if \\((\\alpha-\\lambda I)^{k}(v)=0\\) and \\((\\alpha-\\lambda I)^{k-1}(v)\\ne 0\\) for some \\(k\\ge 1\\) where \\(k\\) is called the degree of nilpotence for \\(v\\).\n\n\nLet \\(\\lambda\\in spec(\\alpha)\\). Then, \\(M_{\\lambda}=\\bigcup\\limits_{m=1} ker(\\lambda I-\\alpha)^m\\) is what we call it the generalized eigenspace corresponding to \\(\\lambda\\). Clearly, \\(M_{\\lambda}\\) is the union of the zero vector and the set of all generalized eigenvectors of \\(\\alpha\\) associated with \\(\\lambda\\)\n\n\nFact: \\(M_{\\lambda}\\) is a subspace and \\(\\alpha-\\)invariant and if \\(v\\) is a generalized vector of index \\(k\\) then \\(\\{v,(\\alpha-\\lambda I)v,\\cdots, (\\alpha-\\lambda I)^{k-1}v\\}\\) is linearly independent.\n\n\nProof: Let \\(a\\in \\mathbb{F}\\) and let \\(v,w\\in V\\) be generalized eigenvectors of \\(\\alpha\\) associated with \\(\\lambda\\) of degrees \\(k\\) and \\(h\\) respectively. Then,\n\n\n\\((\\alpha-\\lambda I)^k(v)=0\\) and \\((\\alpha-\\lambda I)^h(w)=0\\)\n\n\n\\(\\implies v\\in ker (\\alpha-\\lambda I)^k\\) and \\(w\\in ker(\\alpha-\\lambda I)^h\\)\n\n\n\\(\\implies v\\in ker (\\alpha-\\lambda I)^{k+h}\\) and \\(w\\in ker(\\alpha-\\lambda I)^{k+h}\\) because \\((\\alpha-\\lambda I)^{k+h}(v)=0\\) and \\((\\alpha-\\lambda I)^{k+h}(w)=0\\)\n\n\n\\(\\implies v+w \\in ker(\\alpha-\\lambda)^{k+h}\\)\n\n\nAnd, \\((\\alpha-\\lambda I)^{k+h}(av)=a.(\\alpha-\\lambda I)^{k+h}(v)=0\\).\n\n\nThis implies that \\(M_{\\lambda}\\) is a subspace of \\(V\\).\n\n\nInvariance: If \\(\\beta\\in End(V)\\) commutes with \\(\\alpha\\) and if \\(v\\) is a generalized eigenvector of \\(\\alpha\\) associated with \\(\\lambda\\) such that \\(v\\in ker(\\alpha-\\lambda I)^k\\) then,\n\n\n\\((\\alpha-\\lambda I)^k\\beta(v)=\\beta(\\alpha-\\lambda I)^k(v)=0_V\\)\n\n\n\\(\\implies \\beta(v)\\) is also a generalized eigenvector of \\(\\alpha\\) associated with \\(\\lambda\\)\n\n\n\n\n\nLinearly Independence: If \\(v\\) is a generalized vector of \\(\\alpha\\) associated with \\(\\lambda\\) then \\((\\alpha-\\lambda I)^k(v)=0\\) and \\((\\alpha-\\lambda I)^{k-1}(v)\\ne 0\\). Now we assume that,\n\n\n\\(c_0v+c_1(\\alpha-\\lambda I)(v)+\\cdots+c_{k-1}(\\alpha-\\lambda I)^{k-1}(v)=0\\).\n\n\nWe need to show that \\(c_i's\\) are zero for \\(0\\le i\\le k-1\\).\n\n\nApplying \\((\\alpha-\\lambda)^{k-1}\\) we get,\n\n\n\\((\\alpha-\\lambda)^{k-1}(c_0v+c_1(\\alpha-\\lambda I)(v)+\\cdots+c_{k-1}(\\alpha-\\lambda I)^{k-1}(v))=0\\)\n\n\n\\(\\implies c_0(\\alpha-\\lambda I)^{k-1}(v)=0\\). Since \\((\\alpha-\\lambda I)^{k-1}(v)\\ne 0\\) so we get \\(c_0=0\\).\n\n\nSimilarly applying \\((\\alpha-\\lambda I)^{k-2},(\\alpha-\\lambda I)^{k-3},\\) and so on, we have\n\n\n\\(c_i=0\\) for \\(1\\le i\\le k-1\\).\n\n\nHence, \\(\\{v,(\\alpha-\\lambda I)v,\\cdots, (\\alpha-\\lambda I)^{k-1}v\\}\\) is linearly independent.\n\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementation of Gradient Descent (GD) and Stochastic Gradient Descent (SGD)\n\n14 min\n\n\nRafiq Islam\n\n\nThursday, September 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nLU Factorization of a Full rank Matrix using Fortran\n\n26 min\n\n\nRafiq Islam\n\n\nTuesday, November 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix Representation: Change of Basis\n\n3 min\n\n\nRafiq Islam\n\n\nThursday, January 21, 2021\n\n\n\n\n\n\nNo matching items\n Back to topCitationBibTeX citation:@online{islam2021,\n  author = {Islam, Rafiq},\n  title = {Generalized Eigenvectors and Eigenspaces},\n  date = {2021-01-25},\n  url = {https://kaizhongmu.github.io/posts/eigen/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nIslam, Rafiq. 2021. “Generalized Eigenvectors and\nEigenspaces.” January 25, 2021. https://kaizhongmu.github.io/posts/eigen/."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nDescription\n\n\n\n\n\n\n\n\nSpring 2025: MAC2311 Calculus With Analytic Geometry I\n\n\n\n\n\n\n\n\n\nSpring 2024: MAP4170 Introduction to Actuarial Mathematics\n\n\n\n\n\n\n\n\n\nFall 2023: MAP4170 Introduction to Actuarial Mathematics\n\n\n\n\n\n\n\n\n\nSpring 2023: MAC1140 PreCalculus Algebra\n\n\n\n\n\n\n\n\n\nFall 2022: MAC2311 Calculus and Analytic Geometry I\n\n\n\n\n\n\n\n\n\nFall 2021 and Spring 2022: PreCalculus and Algebra\n\n\n\n\n\n\n\n\n\nFall 2018 to Spring 2020: College Algebra, Trigonometry\n\n\n\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "index.html#kaizhong-mike-mu",
    "href": "index.html#kaizhong-mike-mu",
    "title": "",
    "section": "Kaizhong (Mike) Mu",
    "text": "Kaizhong (Mike) Mu\nSc.M. in Biostatistics  Brown University\nContact\nEmail:  Kaizhong_mu@brown.edu"
  },
  {
    "objectID": "index.html#i-am",
    "href": "index.html#i-am",
    "title": "",
    "section": "I am",
    "text": "I am\n\nI am Statistics & AI/ML enthusiast, bridging statistical rigor with data science practice  I am an independent researcher, aiming to develop new AI/ML+Stats methods  I am from a family of doctors, which sparked my interest in solving real-world biomedical challenge.  I am a sailor. I am a Roadtripper. I am a thinker. I am ……."
  },
  {
    "objectID": "index.html#i-believe",
    "href": "index.html#i-believe",
    "title": "",
    "section": "I believe",
    "text": "I believe\n\nStatistics and AI/ML are stand in a yin-yang relationship:\nAI’s black-box nature requires statistical theory to increase interpretability, while statistics methodology should embrace data-driven technique to boost practical impact.   Good researchers should hold themselves to higher standards:\nmust deeply understand the mathematical principles and statistical meaning behind models, stay open to new techniques and theories, quiet devotion to creating something both rigorous.   Nature has much to teach us:\nabout science, about love, about the beauty of simplicity."
  },
  {
    "objectID": "banglablog/welcomepost/index.html",
    "href": "banglablog/welcomepost/index.html",
    "title": "বাংলা ব্লগে আপনাকে স্বাগতম",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "CourseAndAward.html",
    "href": "CourseAndAward.html",
    "title": "Course",
    "section": "",
    "text": "Bayesian Inference\nAnalysis of Lifetime Data\nCasual Inference and Missing Data\nMultivariate Data Analysis\nCategorical Data Analysis\nTime Series Analysis\nAnalysis of Variance\nApplied Generalized Linear Models\nRegression Analysis\nPractical Data Analysis\nAnalysis of Economic Data\nGame Theory\nIntroduction to Biology\nFundamentals of Probability & Statistical Inference\nMathematical Statistics 2\nMathematical Statistics 1\nLinear Algebra\nCalculus 1, 2, 3\nStatistical Learning and Big Data\nFundamentals of Statistical Data Science (R)\nStatistical Programming with R\nData & Web Technologies for Data Analysis (Python)\nPython Programming\nMATLAB Programming"
  },
  {
    "objectID": "CourseAndAward.html#course",
    "href": "CourseAndAward.html#course",
    "title": "Course",
    "section": "",
    "text": "Bayesian Inference\nAnalysis of Lifetime Data\nCasual Inference and Missing Data\nMultivariate Data Analysis\nCategorical Data Analysis\nTime Series Analysis\nAnalysis of Variance\nApplied Generalized Linear Models\nRegression Analysis\nPractical Data Analysis\nAnalysis of Economic Data\nGame Theory\nIntroduction to Biology\nFundamentals of Probability & Statistical Inference\nMathematical Statistics 2\nMathematical Statistics 1\nLinear Algebra\nCalculus 1, 2, 3\nStatistical Learning and Big Data\nFundamentals of Statistical Data Science (R)\nStatistical Programming with R\nData & Web Technologies for Data Analysis (Python)\nPython Programming\nMATLAB Programming"
  },
  {
    "objectID": "CourseAndAward.html#award-honor",
    "href": "CourseAndAward.html#award-honor",
    "title": "Course",
    "section": "Award & Honor",
    "text": "Award & Honor\n\nHealth Data Science Summer Fellowship\nBrown University — Fellow & RA (Award: $5,000)\nSelected to attend health data science training and conduct research under an assigned PI.\n\n\nScholarship\nSchool of Public Health, Brown University — (Award: $9,941)\n20% tuition reduction\n\n\nDean’s Honor List\nUniversity of California, Davis — 2021, 2022, 2023\nTop academic performance student\n\n\nBoyd Harshburger Travel Award Winner\nSROCS Conference — 2025\nThe abstract and poster presentation of the project win prize in Southern Regional Council On Statistics (SRCOS)"
  },
  {
    "objectID": "posts/mathbiology/index.html",
    "href": "posts/mathbiology/index.html",
    "title": "Modeling viral disease",
    "section": "",
    "text": "Consider the spreading of a highly communicable disease on an isolated island with population size \\(N\\). A portion of the population travels abroad and returns to the island infected with the disease. You would like to predict the number of people \\(X\\) who will have been infected by some time \\(t\\). Consider the following model, where \\(k &gt; 0\\) is constant:\n\\[\\begin{equation*}\n  \\frac{dX}{dt}=k\\textcolor{red}{X}(N-X)\n\\end{equation*}\\]\n\nList two major assumptions implicit in the preceding model. How reasonable are your assumptions?\nAnswer: Here are two major assumptions:\n\n\n\nFixed population \\(\\implies\\) all infected. We assume the population size remain unchanged that is no one gets in the island or no one gets out of the island. This will lead everyone affected by the disease eventually.\nNo immediate cure or vaccination. We also assume that there is no immediate hard immunity build up among the population or invention of vaccination.\n\n\nGraph \\(\\frac{dX}{dt}\\) versus \\(X\\)\n\n\n\n\nPhoto\n\n\n\nGraph \\(X\\) versus \\(t\\) if the initial number of infections is \\(X_1 &lt; \\frac{N}{2}\\). Graph \\(X\\) versus \\(t\\) if the initial number of infections is \\(X_2 &gt;\\frac{N}{2}\\).\nAnswer: For equilibrium of the model\n\n\\[\\begin{align*}\n  f(X)&=kX(N-X)=0\\\\\n  \\implies kX&=0 & N-X=0\\\\\n  \\implies X=&0  & X=N\n\\end{align*}\\] For the stability analysis:\n\\[\\begin{align*}\nf(X)&=(kN)X-kX^2 & \\implies f'(X)=kN-2kX\n\\end{align*}\\]\nNow, \\(f'(0)=kN&gt;0\\) therefore, \\(X=0\\) is an unstable equilibrium. And \\(f'(N)=kN-2kN=-kN&lt;0\\) since \\(k, N&gt;0\\). So, \\(X=N\\) is a stable equilibrium.\n\n\n\nequilibrium\n\n\nIf the initial infection \\(X_1&lt;\\frac{N}{2}\\) it might decrease and reach to 0 but that is not a stable equilibrium. So eventually it will hit \\(N\\).\n\nSolve the model given earlier for \\(X\\) as a function of \\(t\\).\nAnswer: Solving the ODE we have\n\n\\[\\begin{align*}\n  \\frac{dX}{dt}&=kX(N-X)\\\\\n  \\text{Since}\\hspace{2mm} X&&gt;0\\\\\n  \\frac{dX}{X(N-X)}&=kdt\\\\\n  \\implies \\int \\frac{dX}{X(N-X)}&=\\int kdt\\\\\n  \\implies \\frac{1}{N}\\int \\left(\\frac{1}{X}+\\frac{1}{N-X}\\right)dX&= \\int kdt\\\\\n  \\implies \\frac{1}{N} \\ln\\left(\\frac{X}{N-X}\\right)&=kt+c\\\\\n  \\implies \\ln\\left(\\frac{X}{N-X}\\right)&=Nkt+Nc\\\\\n  \\implies \\frac{X}{N-X}&=e^{Nkt+Nc}\\\\\n  \\implies X&=Ne^{Nkt+Nc}-Xe^{Nkt+Nc}\\\\\n  \\implies X\\left(1+e^{Nkt+Nc}\\right)&=Ne^{Nkt+Nc}\\\\\n  \\implies X(t)&=\\frac{Ne^{Nkt+Nc}}{1+e^{Nkt+Nc}}\\\\\n  \\implies X(t)&=\\frac{N}{1+e^{-(Nkt+Nc)}}\n\\end{align*}\\]\n\nFrom part (d), find the limit of \\(X\\) as \\(t\\) approaches infinity.\nAnswer:\n\n\\[\\begin{align*}\n\\lim_{t\\longrightarrow \\infty} X(t)&=\\lim_{t\\longrightarrow \\infty} \\frac{N}{1+e^{-(Nkt+Nc)}}=N\n\\end{align*}\\]\n\nConsider an island with a population of \\(5000\\). At various times during the epidemic the number of people infected was recorded as follows:\n\n\n\n\n\\(t\\) (days)\n2\n6\n\n\n\n\n\\(X\\) (People infected)\n\\(1887\\)\n\\(4087\\)\n\n\n\\(\\ln{\\left(\\frac{X}{N-X}\\right)}\\)\n\\(-0.5\\)\n\\(1.5\\)\n\n\n\nDo the collected data support the given model?\nAnswer: If we look at part (d) we have\n\\[\\begin{align*}\n\\ln\\left(\\frac{X}{N-X}\\right)&=Nkt+Nc & \\text(And),\\\\\nX(t)&=\\frac{N}{1+e^{-(Nkt+Nc)}}\n\\end{align*}\\]\nSo we get if \\(2Nk+Nc=-0.5\\) then \\(X(2)=\\frac{5000}{1+e^{0.5}}=1887.703\\), if \\(6Nk+Nc=1.5\\) then \\(X(6)=\\frac{5000}{e^{-1.5}}=4087.87\\), and if \\(10Nk+Nc=3.5\\) then \\(X(10)=\\frac{5000}{e^{-3.5}}=4853.44\\)\nTherefore, the collected data supports the model.\n\nUse the results in part (f) to estimate the constants in the model, and predict the number of people who will be infected by \\(t = 12\\) days.\nAnswer: We have\n\n\\[\\begin{align*}\n2Nk+Nc&=-0.5\\\\\n6Nk+Nc&=1.5\n\\end{align*}\\]\nSolving the above system we have \\(k=\\frac{1}{2N}\\) and \\(c=\\frac{-1.5}{N}\\). If we substitute these values in the solution we got in part (d) we have\n\\[\\begin{align*}\n  X(t)&=\\frac{N}{1+e^{-\\left(\\frac{t}{2}-1.5\\right)}}\n\\end{align*}\\]\nSo, \\(X(12)\\approx 4945\\)\n\n\n\n\n\n\n\n\nShare on\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized eigenvectors and eigenspaces\n\n2 min\n\n\nRafiq Islam\n\n\nMonday, January 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementation of Gradient Descent (GD) and Stochastic Gradient Descent (SGD)\n\n14 min\n\n\nRafiq Islam\n\n\nThursday, September 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nLU Factorization of a Full rank Matrix using Fortran\n\n26 min\n\n\nRafiq Islam\n\n\nTuesday, November 9, 2021\n\n\n\n\n\n\nNo matching items\n Back to topCitationBibTeX citation:@online{islam2021,\n  author = {Islam, Rafiq},\n  title = {Modeling Viral Disease},\n  date = {2021-02-23},\n  url = {https://kaizhongmu.github.io/posts/mathbiology/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nIslam, Rafiq. 2021. “Modeling Viral Disease.” February 23,\n2021. https://kaizhongmu.github.io/posts/mathbiology/."
  },
  {
    "objectID": "posts/someproofs/index.html",
    "href": "posts/someproofs/index.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "posts/someproofs/index.html#let-n-be-a-positive-integer.-show-that-every-matrix-a-in-m_n-times-nmathbbr-can-be-written-as-the-sum-of-two-non-singular-matrices.",
    "href": "posts/someproofs/index.html#let-n-be-a-positive-integer.-show-that-every-matrix-a-in-m_n-times-nmathbbr-can-be-written-as-the-sum-of-two-non-singular-matrices.",
    "title": "",
    "section": "1. Let \\(n\\) be a positive integer. Show that every matrix \\(A \\in M_{n \\times n}(\\mathbb{R})\\) can be written as the sum of two non-singular matrices.",
    "text": "1. Let \\(n\\) be a positive integer. Show that every matrix \\(A \\in M_{n \\times n}(\\mathbb{R})\\) can be written as the sum of two non-singular matrices.\nProof: To prove this, we will use two known properties of matrices.\n\n\\(det(A)=\\) Product of the eigenvalues of \\(A\\)\n\nIf \\(\\lambda\\) is an eigenvalue of \\(A\\) then \\(\\lambda+n\\) is an eigenvalue of \\(A+nI\\) matrix.\n\nSince, \\(A\\in M_{n\\times n}(\\mathbb{R}),\\) let \\(\\lambda_i\\) for \\(1\\le i\\le n\\) be the eigenvalues of \\(A\\). The matrix \\(A+(n+1)I\\) has eigenvalues \\(\\lambda_{i+n+1}\\) for \\(1\\le i\\le n\\).\nLet,\n\\[\\begin{align*}\nn&=max\\{|\\lambda_i| : 1\\le i \\le n\\}\\\\\n\\implies& -n\\le \\lambda_i \\le n \\text{ for all }1\\le i \\le n\\\\\n\\implies& -n+n+1\\le \\lambda_i+n+1 \\le n+n+1\\text{ for all }1\\le i \\le n\\\\\n\\implies& 1\\le \\lambda_i+n+1 \\le 2n+1\\text{ for all }1\\le i \\le n\n\\end{align*}\\] Thus, \\(\\lambda_i+n+1\\ge 1\\) that is \\(\\lambda_i+n+1 \\ne 0\\) and \\(0\\) is not an eigenvalue of \\(A\\).\nNow, from property (1), we have,\n\\(det(A)=\\prod_{i=1}^{n}\\lambda_i\\)\nand\n\\[\\begin{align*}\ndet(A+(n+1)I)&=\\prod_{i=1}^{n}(\\lambda_i+n+1)\\ne 0\\\\\n\\end{align*}\\] \\(\\implies A\\text{ or }A+(n+1)I\\) both are non-singular.\n\\(-(n+1)I\\) is of course non-singular.\nThen\n\\(A=(A+(n+1)I)+(-(n+1)I)\\)"
  },
  {
    "objectID": "posts/someproofs/index.html#let-alpha-in-mathcallv-and-dim-vn-infty",
    "href": "posts/someproofs/index.html#let-alpha-in-mathcallv-and-dim-vn-infty",
    "title": "",
    "section": "2. Let \\(\\alpha \\in \\mathcal{L}(V)\\) and \\(\\dim V=n< \\infty\\)",
    "text": "2. Let \\(\\alpha \\in \\mathcal{L}(V)\\) and \\(\\dim V=n&lt; \\infty\\)\n\nSuppose that \\(\\alpha\\) has two distinct eigenvalues \\(\\lambda\\) and \\(\\mu\\). Prove that if \\(\\dim E_{\\lambda}=n-1\\) then \\(\\alpha\\) is diagonalizable.\n\n\nProof: Since \\(\\mu\\) and \\(\\lambda\\) are two distinct eigenvalues associated with \\(\\alpha\\), so \\(V=E_{\\lambda}\\bigoplus E_{\\mu}\\) and \\(\\dim E_{\\lambda}(\\alpha)+\\dim E_{\\mu}(\\alpha)=n\\).\n\n\nHere, \\(\\dim E_{\\mu}(\\alpha)\\ge 1\\) and \\(\\dim E_{\\lambda}(\\alpha)=n-1\\). So,\n\n\n\\(\\dim E_{\\lambda}(\\alpha)+\\dim E_{\\mu}(\\alpha)=n-1+1=n\\)"
  },
  {
    "objectID": "posts/someproofs/index.html#let-alphain-mathcallv-and-0ne-vin-v-where-dim-vn-infty.",
    "href": "posts/someproofs/index.html#let-alphain-mathcallv-and-0ne-vin-v-where-dim-vn-infty.",
    "title": "",
    "section": "3. Let \\(\\alpha\\in \\mathcal{L}(V)\\) and \\(0\\ne v\\in V\\) where \\(\\dim V=n< \\infty\\).",
    "text": "3. Let \\(\\alpha\\in \\mathcal{L}(V)\\) and \\(0\\ne v\\in V\\) where \\(\\dim V=n&lt; \\infty\\).\n\n\nProve that there is a unique monic polynomial \\(p(t)\\) of the smallest degree such that \\(p(\\alpha)(v)=0\\)\n\n\nProof: Since \\(V\\) is finite-dimensional so there exists smallest \\(k\\) such that \\(\\{v,\\alpha(v),\\cdots,\\alpha^{k-1}(v)\\}\\) is linearly independent but \\(\\{v,\\alpha(v),\\cdots,\\alpha^{k-1}(v),\\alpha^k(v)\\}\\) is linearly dependent. So there exists \\(c_0,c_1,\\cdots,c_k\\in \\mathbb{F}\\) not all zero such that\n\n\n\\(c_0v+c_1\\alpha(v)+c_2\\alpha^2(v)+\\cdots+c_{k-1}\\alpha^{k-1}(v)+c_k\\alpha^k(v)=0\\)\n\n\nWithout loss of generality, let’s assume that \\(c_k\\ne 0\\). Then\n\n\n\\(a_0v+a_1\\alpha(v)+a_2\\alpha^2(v)+\\cdots+a_{k-1}\\alpha^{k-1}(v)+\\alpha^k(v)=0\\)\n\n\nwhere, \\(a_i=\\frac{c_i}{c_k}\\) for \\(1\\le i \\le k\\).\n\n\nThus, \\(p(t)=a_0+a_1t+a_2t^2+\\cdots+a_{k-1}t^{k-1}+t^k\\), a unique monic polynomial such that \\(p(\\alpha)(v)=0\\)\n\n\n\n\n\n(ii) Prove that \\(p(t)\\) from (i) divides the minimal polynomial of \\(\\alpha\\)\n\n\nProof: By polynomial division we have,\n\n\n\\(m(t)=p(t)h(t)+r(t)\\) where, \\(m(t)\\) is the minimal polynomial.\n\n\nThen, \\(m(\\alpha)(v)=p(\\alpha)h(\\alpha)(v)+r(\\alpha)(v)\\)\n\n\n\\(\\implies 0=0+r(\\alpha)(v)\\)\n\n\n\\(\\implies r(\\alpha)(v)=0\\)"
  },
  {
    "objectID": "posts/someproofs/index.html#let-abin-m_ntimes-nmathbbf-such-that-there-exists-an-invertible-matrix-sin-m_ntimes-nmathbbf-such-that-sas-1-and-sbs-1-are-upper-triangular-matrices.-show-that-every-eigenvalue-of-ab-ba-is-zero",
    "href": "posts/someproofs/index.html#let-abin-m_ntimes-nmathbbf-such-that-there-exists-an-invertible-matrix-sin-m_ntimes-nmathbbf-such-that-sas-1-and-sbs-1-are-upper-triangular-matrices.-show-that-every-eigenvalue-of-ab-ba-is-zero",
    "title": "",
    "section": "4. Let \\(A,B\\in M_{n\\times n}(\\mathbb{F})\\) such that there exists an invertible matrix \\(S\\in M_{n\\times n}(\\mathbb{F})\\) such that \\(SAS^{-1}\\) and \\(SBS^{-1}\\) are upper triangular matrices. Show that every eigenvalue of \\(AB-BA\\) is zero",
    "text": "4. Let \\(A,B\\in M_{n\\times n}(\\mathbb{F})\\) such that there exists an invertible matrix \\(S\\in M_{n\\times n}(\\mathbb{F})\\) such that \\(SAS^{-1}\\) and \\(SBS^{-1}\\) are upper triangular matrices. Show that every eigenvalue of \\(AB-BA\\) is zero\n\nProof: To prove the above statement, it is enough to show that \\(spec(AB-BA)=\\{0\\}\\)\n\n\nWe know that if \\(C\\) and \\(D\\) are upper triangular matrices then \\(spec(CD-DC)=\\{0\\}\\). Now let’s assume that \\(C=SAS^{-1}\\) and \\(D=SBS^{-1}\\). Then,\n\n\n\\(CD-DC=SAS^{-1}SBS^{-1}-SBS^{-1}SAS^{-1}\\)\n\n\n\\(\\implies CD-DC=SABS^{-1}-SBAS^{-1}\\)\n\n\n\\(\\implies CD-DC=S(AB-BA)S^{-1}\\)\n\n\nHence \\(CD-DC\\) and \\(AB-BA\\) are similar matrices. So they have the same eigenvalues, that is \\(spec(AB-BA)=\\{0\\}\\)."
  },
  {
    "objectID": "posts/someproofs/index.html#caley-hamilton-theorem",
    "href": "posts/someproofs/index.html#caley-hamilton-theorem",
    "title": "",
    "section": "5. Caley-Hamilton Theorem",
    "text": "5. Caley-Hamilton Theorem\n\nTheorem: Let \\(p(t)\\) be the characteristic polynomial of a matrix \\(A\\). Then \\(p(A)=0\\)\n\n\nBefore we start proving the theorem, we need to discuss some basics.\n\n\nFor Linear Operator: If \\(\\alpha\\in \\mathcal{L}(V)\\) and \\(A=\\Phi_{BB}(\\alpha)\\) is a representation matrix of \\(\\alpha\\) with respect to the basis \\(B\\), then \\(p(A)\\) is the representation matrix of \\(p(\\alpha)\\). Thus we also have \\(p(\\alpha)=0\\) if \\(p\\) is the characteristic polynomial of \\(\\alpha\\)\n\n\nAdjoint Matrix Method: If we have a matrix \\(A=[a_{ij}]\\in M_{n\\times n}(\\mathbb{F})\\) then we define the \\(\\textit{adjoint}\\) of \\(A\\) to be the matrix \\(adj(A)=[b_{ij}]\\in M_{n\\times n}(\\mathbb{F})\\), where \\(b_{ij}=(-1)^{i+j}|A_{ji}|\\) for all \\(1 \\le i,j \\le n\\)\n\n\nAnd, \\(A_{ji}\\) is the matrix obtained by deleting the i-th row and j-th column.\n\n\nExample: Let \\(A=\\left(\\begin{array}{ccc}1 &4 &7\\\\2 &5 &8\\\\3 &6 &9\\end{array}\\right)\\)\n\n\n\\(b_{11}=(-1)^{1+1}|A_{11}|=det\\left(\\begin{array}{cc}5 &8\\\\6 &9\\end{array}\\right)=-3\\)\n\n\n\\(b_{12}=(-1)^{1+2}|A_{21}|=-det\\left(\\begin{array}{cc}4 &7\\\\6 &9\\end{array}\\right)=6\\)\n\n\n\\(b_{13}=(-1)^{1+3}|A_{21}|=det\\left(\\begin{array}{cc}4 &7\\\\5 &8\\end{array}\\right)=-3\\)\n\n\n\\(b_{21}=(-1)^{2+1}|A_{12}|=-det\\left(\\begin{array}{cc}2 &8\\\\3 &9\\end{array}\\right)=6\\)\n\n\n\\(b_{22}=(-1)^{2+2}|A_{22}|=det\\left(\\begin{array}{cc}1 &7\\\\3 &9\\end{array}\\right)=-12\\)\n\n\n\\(b_{23}=(-1)^{2+3}|A_{32}|=-det\\left(\\begin{array}{cc}1 &7\\\\2 &8\\end{array}\\right)=6\\)\n\n\n\\(b_{31}=(-1)^{3+1}|A_{13}|=det\\left(\\begin{array}{cc}2 &5\\\\3 &6\\end{array}\\right)=-3\\)\n\n\n\\(b_{32}=(-1)^{3+2}|A_{23}|=-det\\left(\\begin{array}{cc}1 &4\\\\3 &6\\end{array}\\right)=6\\)\n\n\n\\(b_{33}=(-1)^{3+3}|A_{33}|=-det\\left(\\begin{array}{cc}1 &4\\\\2 &5\\end{array}\\right)=-3\\)\n\n\nThus,\n\n\n\\(adj(A)=\\left(\\begin{array}{ccc}-3 &6 &-3\\\\6 &-12 &6\\\\-3 &6 &-3\\end{array}\\right)\\).\n\n\nThe important formula that we are going to use is that,\n\n\n\\(AA^{-1}=I \\implies A\\frac{adj(A)}{det(A)}=I \\implies A.adj(A)=det(A).I\\) (*)\n\n\n\n\n\nProof: Let \\(A\\in M_{n\\times n}(\\mathbb{F})\\) have the minimal polynomial \\(p(t)=t^n+\\sum_{i=0}^{n-1}a_it^i\\).\n\n\nNow let, \\(adj(tI-A)=[g_{ij}(t)]=\\left(\\begin{array}{cccc}g_{11}(t) &g_{12}(t) &\\cdots &g_{1n}(t)\\\\g_{21}(t) &g_{22}(t) &\\cdots &g_{2n}(t)\\\\\\vdots &\\vdots &\\ddots &\\vdots\\\\g_{n1}(t) &g_{n2}(t) &\\cdots &g_{nn}(t)\\end{array}\\right)\\) be the adjoint matrix of \\((tI-A)\\).\n\n\nSince each \\(g_{ij}(t)\\) is a polynomial of degree at most \\(n-1\\), we can write this as, \\(adj(tI-A)=\\sum_{i=1}^{n}B_it^{n-i}\\) where \\(B_i\\in M_{n\\times n}(\\mathbb{F})\\).\n\n\nThen by (*) we have,\n\n\\[\\begin{align*}\np(t)I&=det(tI-A).I=(tI-A)adj(tI-A)=(tI-A)\\sum_{i=1}^{n}B_it^{n-i}\\\\\n\\implies& (a_0+a_1t+a_2t^2+\\cdots+a_{n-1}t^{n-1}+t^n)I=(tI-A)B_1t^{n-1}+\\cdots+(tI-A)B_{n-1}t+(tI-A)B_n\\\\\n\\implies& (a_0+a_1t+a_2t^2+\\cdots+a_{n-1}t^{n-1}+t^n)I=B_1t^n-AB_1t^{n-1}+B_2t^{n-1}-AB_2t^{n-2}+\\cdots+B_{n-1}t^2-AB_n\n\\end{align*}\\]\n\nBy comparing the coefficients, we get\n\n\n\\(B_1=I\\)\n\n\n\\(B_2-AB_1=a_{n-1}I\\)\n\n\n\\(B_3-AB_2=a_{n-2}I\\)\n\n\n\\(\\vdots\\)\n\n\n\\(B_n-AB_{n-1}=a_1I\\)\n\n\n\\(-AB_n=a_0I\\)\n\n\nNow multiply \\(A^{n+1-j}\\) to the \\(j-th\\) equation, and then sum up both sides we get,\n\n\n\\(A^{n+1-1}B_1=IA^{n+1-1}\\hspace{2.3in} \\implies A^nB_1=A^n\\)\n\n\n\\(A^{n+1-2}(B_2-AB_1)=a_{n-1}A^{n+1-2}\\hspace{1in} \\implies A^{n-1}B_2-AB_1=a_{n-1}A^{n-1}\\)\n\n\n\\(A^{n+1-3}(B_3-AB_2)=a_{n-2}A^{n+1-3}\\hspace{1in} \\implies A^{n-2}B_3-A^{n-1}B_2=a_{n-1}A^{n-2}\\)\n\n\n\\(\\vdots\\hspace{5in} \\vdots\\)\n\n\n\\(A^{n+1-n}(B_n-AB_{n-1})=a_1A^{n+1-n}\\hspace{1in} \\implies AB_n-A^2B_{n-1}=a_1A\\)\n\n\n\\(-AB_n=a_0I\\hspace{3.2in}\\implies -AB_n=a_0I\\)\n\n\nIf we add both sides then we obtain, \\(p(A)=0\\)"
  },
  {
    "objectID": "posts/someproofs/index.html#prove-that-the-spectral-radius-of-the-textitmarkov-matrix-is-less-than-or-equal-to-1",
    "href": "posts/someproofs/index.html#prove-that-the-spectral-radius-of-the-textitmarkov-matrix-is-less-than-or-equal-to-1",
    "title": "",
    "section": "6. Prove that the spectral radius of the \\(\\textit{Markov}\\) matrix is less than or equal to 1",
    "text": "6. Prove that the spectral radius of the \\(\\textit{Markov}\\) matrix is less than or equal to 1\n\nWe need to prove that if \\(A\\) is a Markov matrix then \\(\\rho(A)\\le 1\\). Now, what is a Markov matrix?\n\n\nMarkov Matrix: A matrix \\(A=[a_{i,j}]_{n\\times n}\\) is called a Markov matrix if \\(a_{i,j}\\ge 0\\) for all \\(1\\le i,j \\le n\\) and \\(\\sum_{j=1}^{n} a_i=1\\), that is the sum of the elements in any row is equal to 1.\n\n\nExample: If we have a matrix like this, \\(A=\\left(\\begin{array}{ccc}0.2 &0.4 &0.4\\\\0.1 &0.4 &0.5\\\\0.9 &0.1 &0\\end{array}\\right)\\) then \\(A\\) is a Markov matrix.\n\n\n\n\n\nProof: Let \\(\\lambda \\in spec(A)\\) and \\(x=\\left(\\begin{array}{c}x_1\\\\x_2\\\\\\vdots\\\\x_n\\end{array}\\right)\\ne 0\\) be a column vector. Then we define, \\(x_h=max\\{|x_i|: 1\\le i \\le n\\}\\) & \\(&gt;0\\). Here we are assuming \\(x_h &gt;0\\) because \\(x\\ne 0\\), as a result at least one of the coordinate of \\(x\\) must be greater than \\(0\\).\n\n\nNow,\n\n\n\\(Ax=\\lambda x=\\left(\\begin{array}{c}\\lambda x_1\\\\ \\lambda x_2 \\\\ \\vdots \\\\ \\lambda x_n\\end{array}\\right)\\)\n\n\n\\(\\implies \\lambda x_h =\\sum_{j=1}^{n} a_{hj}x_j\\)\n\n\n\\(\\implies |\\lambda x_h|=|\\lambda |.|x_h|=|\\sum_{j=1}^{n} a_{hj}x_j|\\le \\sum_{j=1}^{n} |a_{hj}| |x_j|\\)\n\n\n\\(\\implies |\\lambda |.|x_h| \\le (\\sum_{j=1}^{n} |a_{hj}|) |x_h|=1. |x_h|\\)\n\n\n\\(\\implies |\\lambda| \\le 1\\)\n\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/sgd/index.html",
    "href": "posts/sgd/index.html",
    "title": "Implementation of Gradient Descent (GD) and Stochastic Gradient Descent (SGD)",
    "section": "",
    "text": "GIF Credit: gbhat.com\n Gradient Descent is an optimization algorithm used to minimize the cost function. The cost function \\(f(\\beta)\\) measures how well a model with parameters \\(\\beta\\) fits the data. The goal is to find the values of \\(\\beta\\) that minimize this cost function. In terms of the iterative method, we want to find \\(\\beta_{k+1}\\) and \\(\\beta_k\\) such that \\(f(\\beta_{k+1})&lt;f(\\beta_k)\\).   For a small change in \\(\\beta\\), we can approximate \\(f(\\beta)\\) using Taylor series expansion\n\\[f(\\beta_{k+1})=f(\\beta_k +\\Delta\\beta_k)\\approx f(\\beta_k)+\\nabla f(\\beta_k)^T \\Delta \\beta_k+\\text{higher-order terms}\\]\n\nThe update rule for vanilla gradient descent is given by:\n\\[\n\\beta_{k+1} = \\beta_k - \\eta \\nabla f(\\beta_k)\n\\]\nWhere:\n\n\\(\\beta_k\\) is the current estimate of the parameters at iteration \\(k\\).\n\\(\\eta\\) is the learning rate, a small positive scalar that controls the step size.\n\\(\\nabla f(\\beta_k)\\) is the gradient of the cost function \\(f\\) with respect to \\(\\beta\\) at the current point \\(\\beta_k\\).\n\n\nThe update rule comes from the idea of moving the parameter vector \\(\\beta\\) in the direction that decreases the cost function the most.\n\nGradient: The gradient \\(\\nabla f(\\beta_k)\\) represents the direction and magnitude of the steepest ascent of the function \\(f\\) at the point \\(\\beta_k\\). Since we want to minimize the function, we move in the opposite direction of the gradient.\nStep Size: The term \\(\\eta \\nabla f(\\beta_k)\\) scales the gradient by the learning rate \\(\\eta\\), determining how far we move in that direction. If \\(\\eta\\) is too large, the algorithm may overshoot the minimum; if it’s too small, the convergence will be slow.\nIterative Update: Starting from an initial guess \\(\\beta_0\\), we repeatedly apply the update rule until the algorithm converges, meaning that the changes in \\(\\beta_k\\) become negligible, and \\(\\beta_k\\) is close to the optimal value \\(\\beta^*\\)."
  },
  {
    "objectID": "posts/sgd/index.html#gradient-descent",
    "href": "posts/sgd/index.html#gradient-descent",
    "title": "Implementation of Gradient Descent (GD) and Stochastic Gradient Descent (SGD)",
    "section": "",
    "text": "GIF Credit: gbhat.com\n Gradient Descent is an optimization algorithm used to minimize the cost function. The cost function \\(f(\\beta)\\) measures how well a model with parameters \\(\\beta\\) fits the data. The goal is to find the values of \\(\\beta\\) that minimize this cost function. In terms of the iterative method, we want to find \\(\\beta_{k+1}\\) and \\(\\beta_k\\) such that \\(f(\\beta_{k+1})&lt;f(\\beta_k)\\).   For a small change in \\(\\beta\\), we can approximate \\(f(\\beta)\\) using Taylor series expansion\n\\[f(\\beta_{k+1})=f(\\beta_k +\\Delta\\beta_k)\\approx f(\\beta_k)+\\nabla f(\\beta_k)^T \\Delta \\beta_k+\\text{higher-order terms}\\]\n\nThe update rule for vanilla gradient descent is given by:\n\\[\n\\beta_{k+1} = \\beta_k - \\eta \\nabla f(\\beta_k)\n\\]\nWhere:\n\n\\(\\beta_k\\) is the current estimate of the parameters at iteration \\(k\\).\n\\(\\eta\\) is the learning rate, a small positive scalar that controls the step size.\n\\(\\nabla f(\\beta_k)\\) is the gradient of the cost function \\(f\\) with respect to \\(\\beta\\) at the current point \\(\\beta_k\\).\n\n\nThe update rule comes from the idea of moving the parameter vector \\(\\beta\\) in the direction that decreases the cost function the most.\n\nGradient: The gradient \\(\\nabla f(\\beta_k)\\) represents the direction and magnitude of the steepest ascent of the function \\(f\\) at the point \\(\\beta_k\\). Since we want to minimize the function, we move in the opposite direction of the gradient.\nStep Size: The term \\(\\eta \\nabla f(\\beta_k)\\) scales the gradient by the learning rate \\(\\eta\\), determining how far we move in that direction. If \\(\\eta\\) is too large, the algorithm may overshoot the minimum; if it’s too small, the convergence will be slow.\nIterative Update: Starting from an initial guess \\(\\beta_0\\), we repeatedly apply the update rule until the algorithm converges, meaning that the changes in \\(\\beta_k\\) become negligible, and \\(\\beta_k\\) is close to the optimal value \\(\\beta^*\\)."
  },
  {
    "objectID": "posts/sgd/index.html#stochastic-gradient-descent-sgd",
    "href": "posts/sgd/index.html#stochastic-gradient-descent-sgd",
    "title": "Implementation of Gradient Descent (GD) and Stochastic Gradient Descent (SGD)",
    "section": "Stochastic Gradient Descent (SGD)",
    "text": "Stochastic Gradient Descent (SGD)\n\nStochastic Gradient Descent is a variation of the vanilla gradient descent. Instead of computing the gradient using the entire dataset, SGD updates the parameters using only a single data point or a small batch of data points at each iteration. The later one we call it mini batch SGD.\n\nSuppose our cost function is defined as the average over a dataset of size \\(n\\):\n\\[\nf(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} f_i(\\beta)\n\\]\nWhere \\(f_i(\\beta)\\) represents the contribution of the \\(i\\)-th data point to the total cost function. The gradient of the cost function with respect to \\(\\beta\\) is:\n\\[\n\\nabla f(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla f_i(\\beta)\n\\]\nVanilla gradient descent would update the parameters as:\n\\[\n\\beta_{k+1} = \\beta_k - \\eta \\nabla f(\\beta_k)\n\\]\nInstead of using the entire dataset to compute the gradient, SGD approximates the gradient by using only a single data point (or a small batch). The update rule for SGD is:\n\\[\n\\beta_{k+1} = \\beta_k - \\eta \\nabla f_{i_k}(\\beta_k)\n\\]\nWhere:\n\n\\(i_k\\) is the index of a randomly selected data point at iteration \\(k\\).\n\\(\\nabla f_{i_k}(\\beta_k)\\) is the gradient of the cost function with respect to the parameter \\(\\beta_k\\), evaluated only at the data point indexed by \\(i_k\\)."
  },
  {
    "objectID": "posts/sgd/index.html#implementation-of-gd",
    "href": "posts/sgd/index.html#implementation-of-gd",
    "title": "Implementation of Gradient Descent (GD) and Stochastic Gradient Descent (SGD)",
    "section": "Implementation of GD",
    "text": "Implementation of GD\n\nIn 1D\nAssume that we have a function \\(f(x)=x^2-3x+\\frac{13}{4}\\) which we want to minimize. Meaning, we want to find \\(x\\) that minimizes the function.\n\n\n\n\n\n\n\n\n\nNext, let’s implement the GD1\n\n# Define the function\ndef f(x):\n    return x**2-3*x+(13/4)\n# Define the gradient function \ndef grad_f(x):\n    return 2*x-3\n\n# Take a random guess from the domain\nlocal_min = np.random.choice(x)\nprint('Initial guess: ',local_min )\n\n# Hyper-parameters \nlearning_rate = 0.01\ntraining_epochs = 200\n\nmodel_params = np.zeros((training_epochs,2))\nfor i in range(training_epochs):\n    grad = grad_f(local_min)\n    local_min =local_min - learning_rate*grad \n    model_params[i,:] = [local_min, grad]\n\nprint('Empirical/estimated local minimum:',local_min)\n\nfig, axs = plt.subplots(1,2, figsize=(8.5,3.8))\nfor i in range(2):\n    axs[i].plot(model_params[:,i],'-')\n    axs[i].set_xlabel('Iteration')\n    axs[i].set_title(f'Final estimated Min: {local_min:.5f}')\naxs[0].set_ylabel('Local Min')\naxs[1].set_ylabel('Derivative')\nplt.show()\n\nInitial guess:  -1.2152152152152151\nEmpirical/estimated local minimum: 1.4522449397717514\n\n\n\n\n\n\n\n\n\nAlternatively, if we want to set a tolerance, this is how we set that\n\n# Take a random guess from the domain\nlocal_min = np.random.choice(x)\nprint('Initial guess:', local_min)\n\n# Hyper-parameters \nlearning_rate = 0.01\ntolerance = 1e-2  # Stop if gradient is smaller than this value\n\n# Lists to store optimization progress\nlocal_min_vals = []\ngrad_vals = []\niteration = 0  # Track the number of iterations\n\n# Gradient Descent Loop (Runs until gradient is small enough)\nwhile True:\n    grad = grad_f(local_min)\n    # Stop when gradient is smaller than tolerance\n    if abs(grad) &lt; tolerance:\n        print(f\"Stopping at iteration {iteration} (grad={grad:.5f})\")\n        break\n    # Update local minimum\n    local_min = local_min - learning_rate * grad  \n    # Store values\n    local_min_vals.append(local_min)\n    grad_vals.append(grad)\n    iteration += 1  # Increment iteration count\n\nprint('Empirical/estimated local minimum:', local_min)\n\n# Convert lists to numpy arrays\nlocal_min_vals = np.array(local_min_vals)\ngrad_vals = np.array(grad_vals)\n\n# Plot Results\nfig, axs = plt.subplots(1, 2, figsize=(8.5, 3.8))\n\naxs[0].plot(local_min_vals, '.', label=\"Local Min Estimate\")\naxs[0].set_xlabel('Iteration')\naxs[0].set_ylabel('Local Min')\naxs[0].legend()\n\naxs[1].plot(grad_vals, '.', label=\"Gradient Value\")\naxs[1].set_xlabel('Iteration')\naxs[1].set_ylabel('Derivative')\naxs[1].legend()\n\nplt.suptitle(f'Final estimated Min: {local_min:.5f}')\nplt.show()\n\nInitial guess: 2.7227227227227226\nStopping at iteration 273 (grad=0.00984)\nEmpirical/estimated local minimum: 1.504920923879927\n\n\n\n\n\n\n\n\n\n\nParametric variation\nLet’s consider a different problem, \\(f(x)=-e^{-\\frac{x^2}{10}}(0.2x\\cos x+\\sin x)\\). We want to find the \\(x\\) values and optimal hyper-parameters that minimizes \\(f(x)\\).\n\nx = np.linspace(-3*np.pi, 3*np.pi, 400)\ny = -np.exp(-(x**2/10))*(0.2*x*np.cos(x)+np.sin(x))\n\ndy = np.exp(-(x**2/10))*((0.04*x**2-1.2)*np.cos(x)+0.4*x*np.sin(x))\nplt.plot(x,y, x, dy)\nplt.legend(['f(x)','df'])\nplt.show()\n\n\n\n\n\n\n\n\nClearly, the global minimum is somewhere in between 0 to 2.5, maybe around 1.25. Now let’s apply GD with varying parameters\n\ndef f(x):\n    f = -np.exp(-(x**2/10))*(0.2*x*np.cos(x)+np.sin(x))\n    return f\ndef df(x):\n    ddx_of_f = np.exp(-(x**2/10))*((0.04*x**2-1.2)*np.cos(x)+0.4*x*np.sin(x))\n    return ddx_of_f\n\nlearning_rate = 0.003\ntraining_epochs = 1000\nlocal_min = np.random.choice(x,1)\nprint('The chosen initial point: ', local_min)\n\nfor i in range(training_epochs):\n    grad = df(local_min)\n    local_min = local_min - learning_rate*grad \n\nplt.plot(x,f(x), x, df(x),'--')\nplt.plot(local_min, df(local_min),'ro')\nplt.plot(local_min, f(local_min),'ro')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.legend(['f(x)','df','f(x) min'])\nplt.title('Iterated local min at x = %s'%local_min[0])\nplt.show()\n\nThe chosen initial point:  [-6.49577428]\n\n\n\n\n\n\n\n\n\nNow let’s see how we can pick the right initial point\n\n# Vary the starting point \nstarting_points = np.linspace(-5,5,50)\nstopping_points = np.zeros(len(starting_points))\n\ntraining_epochs = 1000\nlearning_rate = 0.01\nfor idx, local_min in enumerate(starting_points):\n    for i in range(training_epochs):\n        grad = df(local_min)\n        local_min = local_min - learning_rate * grad \n    stopping_points[idx] = local_min\nplt.plot(starting_points, stopping_points, 's-')\nplt.xlabel('Starting points')\nplt.ylabel('Stopping points')\nplt.show()\n\n\n\n\n\n\n\n\nBased on this, if we start with any point roughly between \\((-1,3.5)\\) we will end up with the global minimum. However, points outside this interval may take the iterative process to other local minima.\n\n# varying learning rates \nlrs = np.linspace(1e-10, 1e-2, 30)\nstopping_points = np.zeros(len(lrs))\n\ntraining_epochs = 1000\n\nfor idx, lr  in enumerate(lrs):\n    # fixed initial point \n    local_min = -0.03\n    for i in range(training_epochs):\n        grad = df(local_min)\n        local_min = local_min - lr*grad \n    stopping_points[idx] = local_min\nplt.plot(lrs, stopping_points, 's-')\nplt.xlabel('learning rates')\nplt.ylabel('Stopping points')\nplt.show()\n\n\n\n\n\n\n\n\nSo this suggests that a learning rate between 0.003 to 0.005 is a good start.\n\nlrs = np.linspace(1e-10, 1e-2, 30)\ntraining_epochs = np.round(np.linspace(10,1000,50))\n\nstopping_points = np.zeros((len(lrs), len(training_epochs)))\n\nfor lr_idx, lr in enumerate(lrs):\n    for tr_idx, tr_epoch in enumerate(training_epochs):\n        local_min = -0.03\n        \n        for i in range(int(tr_epoch)):\n            grad = df(local_min)\n            local_min = local_min - lr * grad \n\n        stopping_points[lr_idx, tr_idx] = local_min\nplt.imshow(stopping_points, extent=[lrs[0], lrs[-1],\\\n     training_epochs[0], training_epochs[-1]], aspect='auto',\\\n        vmin=0, vmax=1.5)\nplt.xlabel('learning rate')\nplt.ylabel('training epochs')\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\nWe know that the expected minimum at \\(x \\approx 1.17\\) so the color corresponding to this is light. Therefore, if we choose a very small learning rate and/or a larger training epoch then we end up to the darker area which is not a good approximation.\n\nplt.plot(lrs, stopping_points)\nplt.xlabel('learning rate')\nplt.ylabel('Iterated solution')\nplt.title('Each line is a training epoch N')\nplt.show\n\n\n\n\n\n\n\n\n\n\n\nIn 2D\n\nLet’s imagine a hypothetical scenario, Walmart Inc. wants to explore their business in a new twon. They want to have their store in location so that the total distance of the store from all the houses in the neighborhood is the smallest possible. If they have the data of \\(n\\) houses with corresponding coordinates of the houses, return the optimized location for the store.\n\nThe Euclidean distance between two points \\((x_1,y_1)\\) and \\((x_2,y_2)\\) is given by\n\\[d=\\sqrt{(x_1-x_2)^2+(y_1-y_2)}\\]\nAssume that \\(P=(x,y)\\) is the coordinate of Walmart. So for a total of \\(n\\) such points the total distance \\(D\\) from the point \\(P\\) is a function of two variable \\(x\\) and \\(y\\) of the following form\n\\[D=f(x,y)=\\sum_{i=1}^{n}\\sqrt{(x-x_i)^2+(y-y_i)^2}\\]\n\nimport plotly.offline as iplot\nimport plotly as py\npy.offline.init_notebook_mode(connected=True)\nimport plotly.graph_objects as go\n\n\ndef f(x,y, c, d):\n    return np.sqrt((x-c)**2+(y-d)**2)\n\nx = np.linspace(-10,10, 400)\ny = np.linspace(-10,10, 400)\nx, y = np.meshgrid(x,y)\n\nc, d = 0,0\n\nz = f(x, y, c, d)\n\nfig = go.Figure(data=[go.Surface(z=z, x=x, y=y)])\nfig.update_layout(\n    title='3D plot',\n    scene=dict(\n        xaxis_title = 'x',\n        yaxis_title = 'y',\n        zaxis_title = 'z'\n    )\n)\n\nfig.show()\n\n                            \n                                            \n\n\nall we need to do is to minimize the function \\(f(x,y)\\) and to do that we need to calculate the gradient vector which is the partial derivative of \\(f(x,y)\\) with respect to \\(x\\) and \\(y\\). So,\n\\[\\begin{align*}\n\\frac{\\partial f}{\\partial x}& = \\sum_{i=1}^{n} \\frac{x-x_i}{\\sqrt{(x-x_i)^2+(y-y_i)^2}}\\\\\n\\frac{\\partial f}{\\partial y}& = \\sum_{i=1}^{n} \\frac{y-y_i}{\\sqrt{(x-x_i)^2+(y-y_i)^2}}\\\\\n& \\\\\n\\implies \\nabla f(x,y) &= \\begin{bmatrix}\\frac{\\partial f}{\\partial x}\\\\\\frac{\\partial f}{\\partial y}\\end{bmatrix}\n\\end{align*}\\]\nThen the algorithm\n\\[\\begin{align*}\n\\begin{bmatrix}x_{i+1}\\\\y_{i+1}\\end{bmatrix}&= \\begin{bmatrix}x_{i}\\\\y_{i}\\end{bmatrix} - \\eta_i \\begin{bmatrix}\\frac{\\partial f}{\\partial x}|_{x_i}\\\\\\frac{\\partial f}{\\partial y}|_{y_i}\\end{bmatrix}\n\\end{align*}\\]\nwhere, the \\(\\eta\\) is the step size or learning rate that scales the size of the move towards the opposite of the gradient direction.\nNext, how do we control the numerical stability of the algorithm? We need to decrease the step size at each iteration which. This is called the rate of decay. We also need a termination factor or tolerance level that determines if we can stop the iteration. Sometimes, for a deep down convex function, the process oscillates back and forth around a range of values. In this case, applying a damping factor increases the chance for a smooth convergence.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\nclass GDdistanceMin:\n    def __init__(self, step_size=1, decay_rate=0.99, tolerance=1e-7, damping_rate=0.75, points=[]):\n        self.step_size = step_size\n        self.decay_rate = decay_rate\n        self.tolerance = tolerance\n        self.damping_rate = damping_rate\n        self.points = points\n        self.x = sum(x for x, y in points) / len(points)  # Initialization\n        self.y = sum(y for x, y in points) / len(points)  # Initialization\n        self.x_updates = []\n        self.y_updates = []\n\n    def _partial_derivative_x(self, x, y):\n        grad_x = 0\n        for xi, yi in self.points:\n            if x != xi or y != yi:\n                grad_x += (x - xi) / math.sqrt((x - xi)**2 + (y - yi)**2)\n        return grad_x\n\n    def _partial_derivative_y(self, x, y):\n        grad_y = 0\n        for xi, yi in self.points:\n            if x != xi or y != yi:\n                grad_y += (y - yi) / math.sqrt((x - xi)**2 + (y - yi)**2)\n        return grad_y\n\n    def gradient_descent(self):\n        dx, dy = 0, 0\n        while self.step_size &gt; self.tolerance:\n            dx = self._partial_derivative_x(self.x, self.y) + self.damping_rate * dx \n            dy = self._partial_derivative_y(self.x, self.y) + self.damping_rate * dy \n            self.x -= self.step_size * dx \n            self.x_updates.append(self.x)\n            self.y -= self.step_size * dy \n            self.y_updates.append(self.y)\n            self.step_size *= self.decay_rate\n        return (self.x, self.y)\n\ndef f(x, y, c, d):\n    return np.sqrt((x - c)**2 + (y - d)**2)\n\n# Define points\npoints = [(1, 3), (-2, 4), (3, 4), (-2, 1), (9, 2), (-5, 2)]\ngd_min = GDdistanceMin(points=points)\nmin_pt = gd_min.gradient_descent()\nxs = gd_min.x_updates\nys = gd_min.y_updates\nprint(\"Minimum point:\", min_pt)\n\nc, d = min_pt\n\n# Create a grid for plotting\nx = np.linspace(-10, 10, 400)\ny = np.linspace(-10, 10, 400)\nx_grid, y_grid = np.meshgrid(x, y)\nz = f(x_grid, y_grid, c, d)\n\n# Calculate z values for the updates\nzs = [f(xi, yi, c, d) for xi, yi in zip(xs, ys)]\n\n# Plotting\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(x_grid, y_grid, z, cmap='viridis', alpha=0.6)\nax.scatter(xs, ys, zs, color='red', s=50, label=\"Updates\", marker='o')\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\nMinimum point: (0.9999997458022071, 2.9999999769909707)\n\n\n\n\n\n\n\n\n\nDifferent approach2\n\nimport sympy as sym \nsym.init_printing()\nfrom IPython.display import display, Math\nfrom matplotlib_inline.backend_inline import set_matplotlib_formats\nset_matplotlib_formats('svg') \n\n\ndef distance_function(x,y,c,d):\n    x,y = np.meshgrid(x,y)\n    z = np.sqrt((x-c)**2+(y-d)**2)\n    return z\n\nc,d = 0,0\nx = np.linspace(-5,5, 400)\ny = np.linspace(-5,5, 400)\nz = distance_function(x,y,c,d)\n\nplt.imshow(z, extent=[x[0],x[-1], y[0],y[-1]],vmin=-5, vmax=5, origin='lower')\nplt.show()\n\n\n\n\n\n\n\n\nSo, the minimum is in the center of the deep color area. Let’s compute the derivatives using sympy library\n\nsx, sy = sym.symbols('sx,sy')\nsz = sym.sqrt((sx-c)**2+(sy-d)**2)\n\ndf_sx = sym.lambdify((sx,sy), sym.diff(sz, sx), 'sympy')\ndf_sy = sym.lambdify((sx,sy), sym.diff(sz, sy), 'sympy')\n\ndisplay(Math(f\"\\\\frac{{\\\\partial f}}{{\\\\partial x}}={sym.latex(sym.diff(sz,sx))}\\\n     \\\\text{{ and }}  \\\\frac{{\\\\partial f}}{{\\\\partial x}}\\\\mid_{{(1,1)}} = {df_sx(1,1).evalf()}\"))\ndisplay(Math(f\"\\\\frac{{\\\\partial f}}{{\\\\partial y}}={sym.latex(sym.diff(sz,sy))} \\\n     \\\\text{{ and }}  \\\\frac{{\\\\partial f}}{{\\\\partial y}}\\\\mid_{{(1,1)}} = {df_sy(1,1).evalf()}\"))\n\n\\(\\displaystyle \\frac{\\partial f}{\\partial x}=\\frac{sx}{\\sqrt{sx^{2} + sy^{2}}}     \\text{ and }  \\frac{\\partial f}{\\partial x}\\mid_{(1,1)} = 0.707106781186548\\)\n\n\n\\(\\displaystyle \\frac{\\partial f}{\\partial y}=\\frac{sy}{\\sqrt{sx^{2} + sy^{2}}}      \\text{ and }  \\frac{\\partial f}{\\partial y}\\mid_{(1,1)} = 0.707106781186548\\)\n\n\nNow let’s implement the GD\n\n# Let's pick a random starting point(uniformly distributed between -2 and 2)\nlocal_min = np.random.rand(2)*4-2\nstart_point = local_min[:] # make a copy\n\nlearning_rate = 0.01\ntraining_epochs = 1000\n\ntrajectory = np.zeros((training_epochs,2))\n\nfor i in range(training_epochs):\n    grad = np.array([\n        df_sx(local_min[0],local_min[1]).evalf(),\n        df_sy(local_min[0],local_min[1]).evalf()\n    ])\n    local_min = local_min - learning_rate*grad\n    trajectory[i,:] = local_min\n\nprint('Starting point ',start_point)\nprint('Local min found ',local_min)\n\nplt.imshow(z, extent=[x[0],x[-1], y[0],y[-1]],vmin=-5, vmax=5, origin='lower')\nplt.plot(start_point[0], start_point[1], 'bs')\nplt.plot(local_min[0],local_min[1],'rs')\nplt.plot(trajectory[:,0],trajectory[:,1], 'b', linewidth=3)\nplt.legend(['random start', 'local min'])\nplt.colorbar()\nplt.show()\n\nStarting point  [-0.23816448 -1.00458953]\nLocal min found  [0.00174506404772787 0.00736076621733588]"
  },
  {
    "objectID": "posts/sgd/index.html#implementation-of-sgd",
    "href": "posts/sgd/index.html#implementation-of-sgd",
    "title": "Implementation of Gradient Descent (GD) and Stochastic Gradient Descent (SGD)",
    "section": "Implementation of SGD",
    "text": "Implementation of SGD\nPersonally, I think this webpage is one of the best resources for learning how to implement SGD.\n\nShare on\n\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/sgd/index.html#footnotes",
    "href": "posts/sgd/index.html#footnotes",
    "title": "Implementation of Gradient Descent (GD) and Stochastic Gradient Descent (SGD)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe implementation is taken from the the Udemy course A Deep Understanding of Deep Learning↩︎\nThe implementation is taken from the the Udemy course A Deep Understanding of Deep Learning↩︎"
  },
  {
    "objectID": "posts/lu/index.html",
    "href": "posts/lu/index.html",
    "title": "LU Factorization of a Full rank Matrix using Fortran",
    "section": "",
    "text": "! This program factors a full rank matrix A into lower triangular (or trapezoidal) L and upper\n! triangular matrix U\nprogram LU_decompostion\n    implicit none\n    !############# #################List of main program variable##################################\n    integer::m,n\n    ! m is the # of rows of the matrix that we are working with\n    ! n is the # of columns that we are working with\n    doubleprecision, allocatable,dimension(:,:)::A,A1\n    ! A is the working matrix, A1 is the original matrix preserved to check correctness of factoring\n    integer,allocatable,dimension(:):: P,Q\n    ! P, Q are the row and column permutation vectors for partial and complete pivoting\n    character::method\n    !################################################################################################\n\n    ! ############################ Open an Input Data File###########################################\n    open(unit=1,file=&quot;data.txt&quot;)\n    ! ###############################################################################################\n\n    ! ################################# Read m, n of the matrix A ###################################\n    write(*,*)&quot;Input the number of rows of the matrix A, m&quot;\n    read(*,*) m\n    write(*,*)&quot;Input the number of columns of the matrix A, n&quot;\n    read(*,*)n\n    ! ##############################################################################################\n\n    ! ########################### Allocate Space ###################################################\n    allocate(A(m,n),A1(m,n),P(m),Q(n))\n    ! ##############################################################################################\n\n    ! Create the matrix A\n    print*,\n    call matrixA(m,n,A,A1)\n    print*,\n\n    !##################################### Choose the method #######################################\n    print*,&quot;What method you want to apply?&quot;\n    print*,&quot;For No Pivot input: N&quot;\n    print*, &quot;For Partial Pivot input: P&quot;\n    print*, &quot;For Complete Pivot input: C&quot;\n    read(*,*) method\n    !###############################################################################################\n\n    ! ############################### Execution of the methods #####################################\n    IF(method.eq.&quot;C&quot;.or.method.eq.&quot;c&quot;) THEN\n        print*, &quot;Complete Pivoting method has been selected&quot;\n        print*,\n        call completePivot(m,n,A,A1,P,Q)\n    ELSE IF(method.eq.&quot;P&quot;.or.method.eq.&quot;p&quot;) then\n        print*,&quot;Partial Pivoting method has been selected&quot;\n        print*,\n        call partialPivot(m,n,A,A1,P)\n    ELSE IF (method.eq.&quot;N&quot;.or.method.eq.&quot;n&quot;) then\n        print*, &quot;No Pivoting method has been selected&quot;\n        print*,\n        call noPivot(m,n,A,A1)\n    END IF\n\nend program\n\nsubroutine matrixA(m,n,A,A1)\n    integer,intent(in)::m,n\n    doubleprecision,dimension(m,n),intent(inout)::A,A1\n    integer::i\n    print*,\n    print*, &quot;This is the provided working matrix&quot;\n    print*\n    do i=1,m\n        read(1,*)A(i,:)\n        A1(i,:)=A(i,:)\n        print*,A(i,:)\n    end do\n    do i=1,n\n        IF(A(i,i)==0) then\n            print*,&quot;A 0 entry was found in the main diagonal.&quot;\n            print*, &quot;Therefore, pivoting is a must required&quot;\n        else if(A(i,i).lt.0.0001) then\n            print*, &quot;WARNING!! Diagonal Element is too small.&quot;\n        END IF\n    end do\nend subroutine\n\nsubroutine completePivot(m,n,A,A1,P,Q)\n    integer,intent(in)::m,n\n    doubleprecision,dimension(m,n),intent(inout)::A,A1\n    integer,dimension(m),intent(out)::P\n    integer,dimension(n),intent(out)::Q\n    integer::i,j,k,row,col\n    doubleprecision::temp\n\n    do k=1,n\n       call max_val(A,m,n,k,row,col)\n       do i=k,n\n            temp=A(i,col)\n            A(i,col)=A(i,k)\n            A(i,k)=temp\n        end do\n        Q(k)=col\n        do j=k,n\n            temp=A(k,j)\n            A(k,j)=A(row,j)\n            A(row,j)=temp\n        end do\n        P(k)=row\n\n        A(k+1:n,k)=A(k+1:n,k)/A(k,k)\n        do j=k+1,n\n            do i=k+1,n\n                A(i,j)=A(i,j)-A(i,k)*A(k,j)\n            end do\n        end do\n    end do\n    print*,\n    print*,&quot;------------------------------------------------------&quot;\n    print*,&quot;         Complete Pivot A=LU factorized array&quot;\n    print*,&quot;-------------------------------------------------------&quot;\n    print*,\n    do i=1,m\n        print*,A(i,:)\n    end do\n    print*,\n    print*, &quot;Permutation vector P=(&quot;,(P(i),i=1,m-1),&quot;)&quot;\n    print*,\n    print*, &quot;Permutation vector Q=(&quot;,(Q(i),i=1,n-1),&quot;)&quot;\n    print*,\n    print*,&quot;*******************************************************&quot;\n    print*,&quot;       Checking Correctness of the factorization&quot;\n    print*,&quot;*******************************************************&quot;\n    print*,\n    call CheckingCompletePivot(m,n,A,A1)\n\nend subroutine\n\n\nsubroutine CheckingCompletePivot(m,n,A,A1)\n    integer,intent(in)::m,n\n    doubleprecision,dimension(m,n),intent(inout)::A,A1\n    doubleprecision,dimension(n,n):: U\n    doubleprecision,dimension(m,n)::L,A2\n    integer::i,j\n\n    do i=1,m\n        do j=1,n\n            if(i.le.j)then\n                L(i,j)=0\n            else if (i.gt.j) then\n                L(i,j)=A(i,j)\n            end if\n            L(i,i)=1\n        end do\n    end do\n    do i=1,n\n        do j=1,n\n            if(i.le.j)then\n                U(i,j)=A(i,j)\n            else\n                U(i,j)=0.0\n            end if\n        end do\n    end do\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*,&quot;  Complete Pivot Upper triangular matrix U&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    do i=1,n\n        print*,U(i,:)\n    end do\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*,&quot;  Complete Pivot Lower triangular matrix L&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    do i=1,m\n        print*,L(i,:)\n    end do\n\n    A2=matmul(L,U)\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*,&quot;              Product of L U=&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    do i=1,m\n        print*,A2(i,:)\n    end do\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*, &quot;  Factoring Accuracy with the Frobenius-Norm&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    print*,&quot;Relative Error=&quot;,Frobenius(m,n,A1-A2)/Frobenius(m,n,A1)\n    print*\n    print*, &quot;Growth Factor=&quot;, Frobenius(m,n,matmul(abs(L),abs(U)))/Frobenius(m,n,A1)\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*, &quot;     Factoring Accuracy with the 1-Norm&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    print*,&quot;Relative Error=&quot;,norm1(m,n,A1-A2)/norm1(m,n,A1)\n    print*\n    print*, &quot;Growth Factor=&quot;, norm1(m,n,matmul(abs(L),abs(U)))/norm1(m,n,A1)\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*, &quot;     Factoring Accuracy with the Infinity-Norm&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    print*,&quot;Relative Error=&quot;,infinityNorm(m,n,A1-A2)/infinityNorm(m,n,A1)\n    print*\n    print*, &quot;Growth Factor=&quot;, infinityNorm(m,n,matmul(abs(L),abs(U)))/infinityNorm(m,n,A1)\nend subroutine\n\nsubroutine partialPivot(m,n,A,A1,P)\n    integer,intent(in)::m,n\n    doubleprecision,dimension(m,n),intent(inout)::A,A1\n    integer,dimension(m),intent(out)::P\n    integer::i,j,k,row\n    doubleprecision::temp\n\n    do k=1,n\n       call max_valP(A,m,n,k,row)\n        do j=k,n\n            temp=A(k,j)\n            A(k,j)=A(row,j)\n            A(row,j)=temp\n        end do\n        P(k)=row\n\n        A(k+1:n,k)=A(k+1:n,k)/A(k,k)\n        do j=k+1,n\n            do i=k+1,n\n                A(i,j)=A(i,j)-A(i,k)*A(k,j)\n            end do\n        end do\n    end do\n    print*,\n    print*,&quot;------------------------------------------------------&quot;\n    print*,&quot;          Partial Pivot A=LU factorized array&quot;\n    print*,&quot;-------------------------------------------------------&quot;\n    print*,\n    do i=1,m\n        print*,A(i,:)\n    end do\n    print*,\n    print*, &quot;Permutation vector P=(&quot;,(P(i),i=1,m-1),&quot;)&quot;\n    print*,\n    print*,&quot;*******************************************************&quot;\n    print*,&quot;       Checking Correctness of the factorization&quot;\n    print*,&quot;*******************************************************&quot;\n    print*,\n    call CheckingPartialPivot(m,n,A,A1)\nend subroutine\n\nsubroutine CheckingPartialPivot(m,n,A,A1)\n    integer,intent(in)::m,n\n    doubleprecision,dimension(m,n),intent(inout)::A,A1\n    doubleprecision,dimension(n,n):: U\n    doubleprecision,dimension(m,n)::L,A2\n    integer::i,j\n\n    do i=1,m\n        do j=1,n\n            if(i.le.j)then\n                L(i,j)=0\n            else if (i.gt.j) then\n                L(i,j)=A(i,j)\n            end if\n            L(i,i)=1\n        end do\n    end do\n    do i=1,n\n        do j=1,n\n            if(i.le.j)then\n                U(i,j)=A(i,j)\n            else\n                U(i,j)=0.0\n            end if\n        end do\n    end do\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*,&quot;  Partial Pivot Upper triangular matrix U&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    do i=1,n\n        print*,U(i,:)\n    end do\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*,&quot;  Partial Pivot Lower triangular matrix L&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    do i=1,m\n        print*,L(i,:)\n    end do\n\n    A2=matmul(L,U)\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*,&quot;       Partial Pivot Product of L U=&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    do i=1,m\n        print*,A2(i,:)\n    end do\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*, &quot; Factoring Accuracy with the Frobenius-Norm&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    print*,&quot;Relative Error=&quot;,Frobenius(m,n,A1-A2)/Frobenius(m,n,A1)\n    print*\n    print*, &quot;Growth Factor=&quot;, Frobenius(m,n,matmul(abs(L),abs(U)))/Frobenius(m,n,A1)\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*, &quot;     Factoring Accuracy with the 1-Norm&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    print*,&quot;Relative Error=&quot;,norm1(m,n,A1-A2)/norm1(m,n,A1)\n    print*\n    print*, &quot;Growth Factor=&quot;, norm1(m,n,matmul(abs(L),abs(U)))/norm1(m,n,A1)\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*, &quot;     Factoring Accuracy with the Infinity-Norm&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    print*,&quot;Relative Error=&quot;,infinityNorm(m,n,A1-A2)/infinityNorm(m,n,A1)\n    print*\n    print*, &quot;Growth Factor=&quot;, infinityNorm(m,n,matmul(abs(L),abs(U)))/infinityNorm(m,n,A1)\nend subroutine\n\nsubroutine noPivot(m,n,A,A1)\n    integer,intent(in)::m,n\n    doubleprecision,dimension(m,n),intent(inout)::A,A1\n    integer::i,j,k\n\n    do k=1,n\n        A(k+1:n,k)=A(k+1:n,k)/A(k,k)\n        do j=k+1,n\n            do i=k+1,n\n                A(i,j)=A(i,j)-A(i,k)*A(k,j)\n            end do\n        end do\n    end do\n    print*,\n    print*,&quot;------------------------------------------------------&quot;\n    print*,&quot;          No Pivot A=LU factorized array&quot;\n    print*,&quot;-------------------------------------------------------&quot;\n    print*,\n    do i=1,m\n        print*,A(i,:)\n    end do\n    print*,\n    print*,&quot;*******************************************************&quot;\n    print*,&quot;       Checking Correctness of the factorization&quot;\n    print*,&quot;*******************************************************&quot;\n    print*,\n    call CheckingNoPivot(m,n,A,A1)\nend subroutine\n\nsubroutine CheckingNoPivot(m,n,A,A1)\n    integer,intent(in)::m,n\n    doubleprecision,dimension(m,n),intent(inout)::A,A1\n    doubleprecision,dimension(n,n):: U\n    doubleprecision,dimension(m,n)::L,A2\n    integer::i,j\n\n    do i=1,m\n        do j=1,n\n            if(i.le.j)then\n                L(i,j)=0\n            else if (i.gt.j) then\n                L(i,j)=A(i,j)\n            end if\n            L(i,i)=1\n        end do\n    end do\n    do i=1,n\n        do j=1,n\n            if(i.le.j)then\n                U(i,j)=A(i,j)\n            else\n                U(i,j)=0.0\n            end if\n        end do\n    end do\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*,&quot;  No Pivot Upper triangular matrix U&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    do i=1,n\n        print*,U(i,:)\n    end do\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*,&quot;  No Pivot Lower triangular matrix L&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    do i=1,m\n        print*,L(i,:)\n    end do\n\n    A2=matmul(L,U)\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*,&quot;       No Pivot Product of L U=&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    do i=1,m\n        print*,A2(i,:)\n    end do\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*, &quot;  Factoring Accuracy with the Frobenius Norm&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    print*,&quot;Relative Error=&quot;,Frobenius(m,n,A1-A2)/Frobenius(m,n,A1)\n    print*\n    print*, &quot;Growth Factor=&quot;, Frobenius(m,n,matmul(abs(L),abs(U)))/Frobenius(m,n,A1)\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*, &quot;     Factoring Accuracy with the 1-Norm&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    print*,&quot;Relative Error=&quot;,norm1(m,n,A1-A2)/norm1(m,n,A1)\n    print*\n    print*, &quot;Growth Factor=&quot;, norm1(m,n,matmul(abs(L),abs(U)))/norm1(m,n,A1)\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*, &quot;     Factoring Accuracy with the Infinity-Norm&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    print*,&quot;Relative Error=&quot;,infinityNorm(m,n,A1-A2)/infinityNorm(m,n,A1)\n    print*\n    print*, &quot;Growth Factor=&quot;, infinityNorm(m,n,matmul(abs(L),abs(U)))/infinityNorm(m,n,A1)\nend subroutine\n\nsubroutine max_val(A,m,n,k,row,col)\n    implicit none\n    integer,intent(in)::m,n,k\n    integer,intent(out)::row,col\n    doubleprecision,dimension(m,n),intent(inout)::A\n    doubleprecision::maximum\n    integer::i,j\n\n    maximum=maxval(A(k:m,k:n))\n\n    do i=k,m\n        do j=k,n\n            if(A(i,j)==maximum) then\n                row=i\n                col=j\n                goto 100\n            end if\n        end do\n    end do\n100 end subroutine\n\nsubroutine max_valP(A,m,n,k,row)\n    implicit none\n    integer,intent(in)::m,n,k\n    integer,intent(out)::row\n    doubleprecision,dimension(m,n),intent(inout)::A\n    doubleprecision::maximum\n    integer::i,j\n\n    maximum=maxval(A(k:m,k))\n\n    do i=k,m\n        if(A(i,k)==maximum) then\n            row=i\n            goto 101\n        end if\n    end do\n101 end subroutine\n\n\nfunction norm1(m,n,A)\n    integer::m,n,i,j\n    doubleprecision,dimension(m,n)::A\n    doubleprecision,dimension(n):: normvector\n    doubleprecision::normval\n\n    normval=0\n    do j=1,n\n        do i=1,m\n            normval=normval+abs(A(i,j))\n        end do\n        normvector(j)=normval\n    end do\n    norm1=maxval(normvector(1:n))\n    return\nend function\n\nfunction infinityNorm(m,n,A)\n    integer::m,n,i,j\n    doubleprecision,dimension(m,n)::A\n    doubleprecision,dimension(n):: normvector\n    doubleprecision:: normval\n\n    normval=0\n    do j=1,n\n        do i=1,m\n            normval=normval+abs(A(i,j))\n            normvector(j)=normval\n        end do\n    end do\n    infinityNorm=maxval(normvector(1:m))\n    return\nend function\n\nfunction Frobenius(m,n,A)\n    integer::m,n,i,j\n    doubleprecision,dimension(m,n)::A\n    doubleprecision:: normval\n\n    normval=0\n    do i=1,m\n        do j=1,n\n            normval=normval+(abs(A(i,j)))**2\n        end do\n    end do\n    Frobenius=sqrt(normval)\n    return\nend function"
  },
  {
    "objectID": "posts/lu/index.html#no-pivot",
    "href": "posts/lu/index.html#no-pivot",
    "title": "LU Factorization of a Full rank Matrix using Fortran",
    "section": "No Pivot",
    "text": "No Pivot\nInput Matrix (Stored in a data file in the same directory where the program file .f90 located)\n8 2 9\n4 9 4\n6 7 9\n\nComand prompt: \n Input the number of rows of the matrix A, m\n3\n Input the number of columns of the matrix A, n\n3\n\n\n This is the provided working matrix\n\n   8.0000000000000000        2.0000000000000000        9.0000000000000000\n   4.0000000000000000        9.0000000000000000        4.0000000000000000\n   6.0000000000000000        7.0000000000000000        9.0000000000000000\n\n What method you want to apply?\n For No Pivot input: N\n For Partial Pivot input: P\n For Complete Pivot input: C\nn\n No Pivoting method has been selected\n\n\n ------------------------------------------------------\n           No Pivot A=LU factorized array\n -------------------------------------------------------\n\n   8.0000000000000000        2.0000000000000000        9.0000000000000000\n  0.50000000000000000        8.0000000000000000      -0.50000000000000000\n  0.75000000000000000       0.68750000000000000        2.5937500000000000\n\n *******************************************************\n        Checking Correctness of the factorization\n *******************************************************\n\n\n ----------------------------------------------\n   No Pivot Upper triangular matrix U\n ----------------------------------------------\n\n   8.0000000000000000        2.0000000000000000        9.0000000000000000\n   0.0000000000000000        8.0000000000000000      -0.50000000000000000\n   0.0000000000000000        0.0000000000000000        2.5937500000000000\n\n ----------------------------------------------\n   No Pivot Lower triangular matrix L\n ----------------------------------------------\n\n   1.0000000000000000        0.0000000000000000        0.0000000000000000\n  0.50000000000000000        1.0000000000000000        0.0000000000000000\n  0.75000000000000000       0.68750000000000000        1.0000000000000000\n\n ----------------------------------------------\n        No Pivot Product of L U=\n ----------------------------------------------\n\n   8.0000000000000000        2.0000000000000000        9.0000000000000000\n   4.0000000000000000        9.0000000000000000        4.0000000000000000\n   6.0000000000000000        7.0000000000000000        9.0000000000000000\n\n ----------------------------------------------\n   Factoring Accuracy with the Frobenius Norm\n ----------------------------------------------\n\n Relative Error=   0.00000000\n\n Growth Factor=   1.02520537\n\n ----------------------------------------------\n      Factoring Accuracy with the 1-Norm\n ----------------------------------------------\n\n Relative Error=           0\n\n Growth Factor=           1\n\n ----------------------------------------------\n      Factoring Accuracy with the Infinity-Norm\n ----------------------------------------------\n\n Relative Error=           0\n\n Growth Factor=           1"
  },
  {
    "objectID": "posts/lu/index.html#partial-pivot",
    "href": "posts/lu/index.html#partial-pivot",
    "title": "LU Factorization of a Full rank Matrix using Fortran",
    "section": "Partial Pivot",
    "text": "Partial Pivot\nInput Matrix (Stored in a data file in the same directory where the program file .f90 located)\n1 2 4\n2 1 3\n3 2 4\n\nComand Prompt: \n Input the number of rows of the matrix A, m\n3\n Input the number of columns of the matrix A, n\n3\n\n\n This is the provided working matrix\n\n   1.0000000000000000        2.0000000000000000        4.0000000000000000\n   2.0000000000000000        1.0000000000000000        3.0000000000000000\n   3.0000000000000000        2.0000000000000000        4.0000000000000000\n\n What method you want to apply?\n For No Pivot input: N\n For Partial Pivot input: P\n For Complete Pivot input: C\np\n Partial Pivoting method has been selected\n\n\n ------------------------------------------------------\n           Partial Pivot A=LU factorized array\n -------------------------------------------------------\n\n   3.0000000000000000        2.0000000000000000        4.0000000000000000\n  0.66666666666666663        1.3333333333333335        2.6666666666666670\n  0.33333333333333331      -0.24999999999999992        1.0000000000000000\n\n Permutation vector P=(           3           3 )\n\n *******************************************************\n        Checking Correctness of the factorization\n *******************************************************\n\n\n ----------------------------------------------\n   Partial Pivot Upper triangular matrix U\n ----------------------------------------------\n\n   3.0000000000000000        2.0000000000000000        4.0000000000000000\n   0.0000000000000000        1.3333333333333335        2.6666666666666670\n   0.0000000000000000        0.0000000000000000        1.0000000000000000\n\n ----------------------------------------------\n   Partial Pivot Lower triangular matrix L\n ----------------------------------------------\n\n   1.0000000000000000        0.0000000000000000        0.0000000000000000\n  0.66666666666666663        1.0000000000000000        0.0000000000000000\n  0.33333333333333331      -0.24999999999999992        1.0000000000000000\n\n ----------------------------------------------\n        Partial Pivot Product of L U=\n ----------------------------------------------\n\n   3.0000000000000000        2.0000000000000000        4.0000000000000000\n   2.0000000000000000        2.6666666666666670        5.3333333333333339\n   1.0000000000000000       0.33333333333333337        1.6666666666666667\n\n ----------------------------------------------\n  Factoring Accuracy with the Frobenius-Norm\n ----------------------------------------------\n\n Relative Error=  0.618016541\n\n Growth Factor=   1.11492395\n\n ----------------------------------------------\n      Factoring Accuracy with the 1-Norm\n ----------------------------------------------\n\n Relative Error=           0\n\n Growth Factor=           1\n\n ----------------------------------------------\n      Factoring Accuracy with the Infinity-Norm\n ----------------------------------------------\n\n Relative Error=           0\n\n Growth Factor=           1"
  },
  {
    "objectID": "posts/lu/index.html#complete-pivot",
    "href": "posts/lu/index.html#complete-pivot",
    "title": "LU Factorization of a Full rank Matrix using Fortran",
    "section": "Complete Pivot",
    "text": "Complete Pivot\nInput Matrix (Stored in a data file in the same directory where the program file .f90 located)\n  \n2 3 4\n4 7 5\n4 9 5\n\nCommand Prompt:\n Input the number of rows of the matrix A, m\n3\n Input the number of columns of the matrix A, n\n3\n\n\n This is the provided working matrix\n\n   2.0000000000000000        3.0000000000000000        4.0000000000000000\n   4.0000000000000000        7.0000000000000000        5.0000000000000000\n   4.0000000000000000        9.0000000000000000        5.0000000000000000\n\n What method you want to apply?\n For No Pivot input: N\n For Partial Pivot input: P\n For Complete Pivot input: C\nc\n Complete Pivoting method has been selected\n\n\n ------------------------------------------------------\n          Complete Pivot A=LU factorized array\n -------------------------------------------------------\n\n   9.0000000000000000        4.0000000000000000        5.0000000000000000\n  0.77777777777777779        2.3333333333333335       0.66666666666666674\n  0.33333333333333331       0.47619047619047616       0.57142857142857140\n\n Permutation vector P=(           3           3 )\n\n Permutation vector Q=(           2           3 )\n\n *******************************************************\n        Checking Correctness of the factorization\n *******************************************************\n\n\n ----------------------------------------------\n   Complete Pivot Upper triangular matrix U\n ----------------------------------------------\n\n   9.0000000000000000        4.0000000000000000        5.0000000000000000\n   0.0000000000000000        2.3333333333333335       0.66666666666666674\n   0.0000000000000000        0.0000000000000000       0.57142857142857140\n\n ----------------------------------------------\n   Complete Pivot Lower triangular matrix L\n ----------------------------------------------\n\n   1.0000000000000000        0.0000000000000000        0.0000000000000000\n  0.77777777777777779        1.0000000000000000        0.0000000000000000\n  0.33333333333333331       0.47619047619047616        1.0000000000000000\n\n ----------------------------------------------\n               Product of L U=\n ----------------------------------------------\n\n   9.0000000000000000        4.0000000000000000        5.0000000000000000\n   7.0000000000000000        5.4444444444444446        4.5555555555555554\n   3.0000000000000000        2.4444444444444442        2.5555555555555554\n\n ----------------------------------------------\n   Factoring Accuracy with the Frobenius-Norm\n ----------------------------------------------\n\n Relative Error=  0.683437467\n\n Growth Factor=   1.00393677\n\n ----------------------------------------------\n      Factoring Accuracy with the 1-Norm\n ----------------------------------------------\n\n Relative Error=           0\n\n Growth Factor=           1\n\n ----------------------------------------------\n      Factoring Accuracy with the Infinity-Norm\n ----------------------------------------------\n\n Relative Error=           0\n\n Growth Factor=           1\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/matmul/index.html",
    "href": "posts/matmul/index.html",
    "title": "Matrix multiplication: Let’s make it less expensive!",
    "section": "",
    "text": "Have you ever wondered why your code takes forever to run? Sometimes a simple code may take significant time because of an inefficient implementation approach. Let’s take a simple example of matrix multiplication, and explore the time and space complexity, specifically focusing on multiplying matrices where one of the matrices is formed as an outer product of a vector with itself.\nMatrix multiplication is a fundamental operation in many areas such as computer graphics, machine learning, and scientific computing. Given two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), the product \\(\\mathbf{AB}\\) or \\(\\mathbf{BA}\\) is a new matrix where each element is computed as the dot product of the corresponding row of \\(\\mathbf{A}\\) and the column of \\(\\mathbf{B}\\) or the other way around.\nConsider the scenario where \\(\\mathbf{A}\\) is an outer product of a column vector \\(\\mathbf{a}\\) with itself, i.e.,\n\\[\\begin{align*}\n\\mathbf{A}=\\mathbf{a} \\mathbf{a}^T&=\\begin{pmatrix}a_1\\\\a_2\\\\\\vdots \\\\a_n\\end{pmatrix}\\begin{pmatrix}a_1&a_2&\\cdots &a_n\\end{pmatrix}\\\\\n&=\\begin{pmatrix}\na_1a_1 & a_1a_2& \\cdots &a_1a_n\\\\\na_2a_1& a_2a_2&\\cdots &a_2a_n\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_na_1 & a_na_2 &\\cdots& a_na_n\\end{pmatrix}\\\\\n\\end{align*}\n\\]\nNow simply, if \\(\\mathbf{B}\\) is another \\(n\\times n\\) matrix, then\n\\[\n\\begin{align*}\n\\mathbf{BA}&=\\begin{pmatrix}\nb_{11} & b_{12} & \\cdots & b_{1n}\\\\\nb_{21} & b_{22} & \\cdots & b_{2n}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nb_{n1} & b_{n2} & \\cdots & b_{nn}\\\\\n\\end{pmatrix}\\begin{pmatrix}\na_1a_1 & a_1a_2& \\cdots &a_1a_n\\\\\na_2a_1& a_2a_2&\\cdots &a_2a_n\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_na_1 & a_na_2 &\\cdots& a_na_n\\end{pmatrix}\n\\end{align*}\n\\]\nLet’s analyze the complexity of this matrix matrix multiplication.\nWorst Case: The worst case scenario would be performing the multiplication naively without exploiting the rank-1 structure. How? When we compute any element in the resultant matrix \\(\\mathbf{BA}\\) or \\(\\mathbf{AB}\\) we precisely perform \\(n\\) multiplication and there are total \\(n^2\\) elements to compute for a matrix of \\(n\\times n\\). This would result in the standard matrix multiplication time complexity of \\(O(n^3)\\).\n\\[\n\\begin{align*}\n\\mathbf{BA}&=\\begin{pmatrix}\nb_{11}a_1a_1+\\cdots+b_{1n}a_na_1& b_{11}a_1a_2+\\cdots+b_{1n}a_na_2 &\\cdots &\nb_{11}a_1a_n+\\cdots+b_{1n}a_na_n\\\\\nb_{21}a_1a_1+\\cdots+b_{2n}a_na_1&b_{21}a_1a_2+\\cdots+b_{2n}a_na_2 &\\cdots &\nb_{21}a_1a_n+\\cdots+b_{2n}a_na_n\\\\\n\\vdots & \\vdots &\\ddots & \\vdots\\\\\nb_{n1}a_1a_1+\\cdots+b_{nn}a_na_1&b_{n1}a_1a_2+\\cdots+b_{nn}a_na_2 &\\cdots &\nb_{n1}a_1a_n+\\cdots+b_{nn}a_na_n\\end{pmatrix}\n\\end{align*}\n\\]\nBest Case: The best case scenario in terms of time complexity occurs when we exploit the structure of \\(\\mathbf{A}\\). Since \\(\\mathbf{A}\\) is a rank-1 matrix, we can simplify the multiplication: \\[\n\\begin{align*}\n\\mathbf{BA}&=\\mathbf{B}\\begin{pmatrix}\na_1a_1 & a_1a_2& \\cdots &a_1a_n\\\\\na_2a_1& a_2a_2&\\cdots &a_2a_n\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_na_1 & a_na_2 &\\cdots& a_na_n\\end{pmatrix}\\\\\n&\\\\\n&=\\mathbf{B}\\begin{pmatrix}a_1 \\mathbf{a} & a_2 \\mathbf{a} &\\cdots a_n \\mathbf{a}\\end{pmatrix}\\\\\n&=\\begin{pmatrix}a_1 \\mathbf{B}\\mathbf{a} & a_2 \\mathbf{B}\\mathbf{a} &\\cdots a_n \\mathbf{B}\\mathbf{a}\\end{pmatrix}\\\\\n&= (\\mathbf{Ba}) a^T\n\\end{align*}\n\\]\nWe break this algorithm in to two steps.\nStep 1: Since \\(\\mathbf{B}\\) is a matrix of \\(n\\times n\\) and \\(\\mathbf{a}\\) is a matrix of \\(n\\times 1\\), therefore \\(\\mathbf{Ba}\\) is a matrix of size \\(n\\times 1\\) or just a vector of size \\(n\\).\n\\[\n\\begin{align*}\n\\mathbf{Ba}&=\\begin{pmatrix}\nb_{11} & b_{12} & \\cdots & b_{1n}\\\\\nb_{21} & b_{22} & \\cdots & b_{2n}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nb_{n1} & b_{n2} & \\cdots & b_{nn}\\\\\n\\end{pmatrix}\\begin{pmatrix}a_1\\\\a_2\\\\ \\vdots \\\\a_n \\end{pmatrix}\\\\\n&\\\\\n&=\\begin{pmatrix}\n    b_{11}a_1+b_{12}a_2+\\cdots b_{1n}a_n\\\\\n    b_{21}a_1+b_{22}a_2+\\cdots b_{2n}a_n\\\\\n    \\vdots\\\\\n    b_{n1}a_1+b_{n2}a_2+\\cdots b_{nn}a_n\\\\\n\\end{pmatrix}\n\\end{align*}\n\\] The matrix \\(\\mathbf{Ba}\\) contains \\(n\\) elements where each element takes \\(n\\) multiplications. Thus, computing \\(\\mathbf{Ba}\\) takes \\(O(n^2)\\) time.\nStep 2: Next, we compute \\((\\mathbf{Ba})\\mathbf{a}^T\\).\n\\[\n\\begin{align*}\n(\\mathbf{Ba})\\mathbf{a}^T&=\\begin{pmatrix}ba_1\\\\ ba_2\\\\ \\vdots\\\\ ba_n \\end{pmatrix}\n\\begin{pmatrix}a_{1}& a_{2}& \\cdots a_{n} \\end{pmatrix}\\\\\n&\\\\\n&=\\begin{pmatrix}\n(ba_1)a_1 & (ba_1)a_2 &\\cdots &(ba_1)a_n\\\\\n(ba_2)a_1 & (ba_2)a_2 &\\cdots &(ba_2)a_n\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n(ba_n)a_1 & (ba_n)a_2 &\\cdots &(ba_n)a_n\\\\\n\\end{pmatrix}\n\\end{align*}\n\\] Forming the outer product of \\(\\mathbf{Ba}\\) and \\(\\mathbf{a}^T\\) also takes \\(O(n^2)\\) time. Thus, the best case time complexity is \\(O(n^2)\\).\nWell, how about the other way around? What’s the optimal strategy for \\(\\mathbf{AB}\\)? We can reach similar results in the following way \\[\n\\begin{align*}\n\\mathbf{AB}&=(\\mathbf{a} \\mathbf{a}^T) \\mathbf{B} = \\mathbf{a} (\\mathbf{a}^T \\mathbf{B})\n\\end{align*}\n\\]\nHere, \\(\\mathbf{a}^T \\mathbf{B}\\) is a row vector of size \\(n\\). Computing \\(\\mathbf{a}^T \\mathbf{B}\\) takes \\(O(n^2)\\) time. Then, multiplying the column vector \\(\\mathbf{a}\\) by the resulting row vector forms an \\(n \\times n\\) matrix, also in \\(O(n^2)\\) time. Thus, the best case time complexity is \\(O(n^2)\\). Note, that \\(\\mathbf{AB}\\ne \\mathbf{BA}\\).\nComparison: So, what’s the big difference? There is a significant difference in two algorithms. In the first algorithm the time complexity is \\(O(n^3)\\) where as in the second algorithm the time complexity is \\(O(n^2)+O(n^2)\\) or \\(2O(n^2)\\) or just \\(C\\hspace{1mm} O(n^2)\\). For example, if \\(n=500\\) then the first algorithm requires 125 million multiplications and the second one just takes 500,000 multiplications which is 250 times faster.\nUnderstanding the structure of the matrices involved in multiplication can significantly optimize the performance of our code. By exploiting the rank-1 structure of the outer product matrix \\(\\mathbf{a} = \\mathbf{a} \\mathbf{a}^T\\), we can reduce the time complexity from \\(O(n^3)\\) to \\(O(n^2)\\) in the best case scenario. This optimization can lead to considerable performance improvements, especially for large matrices.\nSpace Complexity: Regardless of the case, the space complexity remains \\(O(n^2)\\) since we need to store the resulting \\(n \\times n\\) matrix \\(\\mathbf{BA}\\).\nPython Code:"
  },
  {
    "objectID": "posts/matmul/index.html#output",
    "href": "posts/matmul/index.html#output",
    "title": "Matrix multiplication: Let’s make it less expensive!",
    "section": "Output",
    "text": "Output\nNaive Multiplication Time: 27.198208 seconds\nOptimized Multiplication Time: 0.001841 seconds\nWhat about when \\(\\mathbf{A}\\) is not given as \\(\\mathbf{A}=\\mathbf{aa}^T\\) (i.e., it’s not a rank-1 matrix)? We simply cannot exploit the same optimization based on the outer product. In this case, we have to use the general matrix multiplication approach, which typically has a time complexity of \\(O(n^3)\\) for naive multiplication. However, there are optimized algorithms that can reduce the time complexity:\n\nStrassen’s Algorithm: Reduces the time complexity to approximately \\(O(n^{2.81})\\)\n\nCoppersmith-Winograd Algorithm: Further reduces the time complexity to approximately \\(O(n^{2.376})\\)\n\nParallel Algorithms: Use parallel computing techniques to perform matrix multiplication more efficiently.\n\nMay be some other day we can talk about these algorithms.\n\n\n\n\n\n\n\n\nShare on\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\nGeneralized eigenvectors and eigenspaces\n\n\n\nLinear Algebra\n\nMathematics\n\n\n\n\n\n\n\n\n\nJan 25, 2021\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nImplementation of Gradient Descent (GD) and Stochastic Gradient Descent (SGD)\n\n\n\nData Science\n\nMachine Learning\n\nStochastic Gradient Descent\n\nOptimization\n\n\n\n\n\n\n\n\n\nSep 19, 2024\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nLU Factorization of a Full rank Matrix using Fortran\n\n\n\nData Science\n\nMachine Learning\n\nComputational Mathematics\n\nAlgorithmic Complexity\n\nProgramming\n\nComputer Science\n\n\n\n\n\n\n\n\n\nNov 9, 2021\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix Representation: Change of Basis\n\n\n\nLinear Algebra\n\nMathematics\n\n\n\n\n\n\n\n\n\nJan 21, 2021\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix multiplication: Let’s make it less expensive!\n\n\n\nData Science\n\nMachine Learning\n\nComputational Mathematics\n\nAlgorithmic Complexity\n\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nModeling viral disease\n\n\n\nApplied Mathematics\n\nMath Biology\n\nMathematical Modeling\n\n\n\n\n\n\n\n\n\nFeb 23, 2021\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nSome Linear Algebra Proofs\n\n\n\nLinear Algebra\n\nMathematics\n\n\n\n\n\n\n\n\n\nJan 24, 2021\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nবাংলা ভাষায় আমার লেখা || My Blog in Benglali Language\n\n\n\n\n\n\n\n\nOct 23, 2024\n\n\nমোহাম্মদ রকিবুল ইসলাম\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "project/dsp/2/index.html",
    "href": "project/dsp/2/index.html",
    "title": "LLM text classification Shiny App (R)",
    "section": "",
    "text": "&lt;p&gt;\nYour browser does not support iframes. &lt;a href=\"https://brown-biostatistics-mikemu.shinyapps.io/automatic_analysis/\" target=\"_blank\"&gt;Open LLM Text Classification App&lt;/a&gt;\n&lt;/p&gt;"
  },
  {
    "objectID": "project/dsp/2/index.html#shiny-app",
    "href": "project/dsp/2/index.html#shiny-app",
    "title": "LLM text classification Shiny App (R)",
    "section": "",
    "text": "&lt;p&gt;\nYour browser does not support iframes. &lt;a href=\"https://brown-biostatistics-mikemu.shinyapps.io/automatic_analysis/\" target=\"_blank\"&gt;Open LLM Text Classification App&lt;/a&gt;\n&lt;/p&gt;"
  },
  {
    "objectID": "project/dsp/2/index.html#data-files",
    "href": "project/dsp/2/index.html#data-files",
    "title": "LLM text classification Shiny App (R)",
    "section": "Data Files",
    "text": "Data Files\nThe app requires three data files:\n\nOriginal Data\nHuman-Filled Data\nGPT-Filled Data"
  },
  {
    "objectID": "project/dsp/2/index.html#shiny-app-readme",
    "href": "project/dsp/2/index.html#shiny-app-readme",
    "title": "LLM text classification Shiny App (R)",
    "section": "Shiny App Readme",
    "text": "Shiny App Readme\nThe goal of this shiny app is to automatically report the comparison of AI vs Human performance in text classification for Private Equity (in Comparison: Human vs. GPT tab) and report the exploratory data analysis (in EDA: Human-filled Dataset tab).\nThis will enable you to no longer need to repeatedly write code/reports when trying various LLM models with human comparisons in the future. All you need to do is input three datasets: “Original Data” (100 entries randomly selected from the original dataset), “Human-Filled Data” (humans text classification), and “GPT-Filled Data” (AI text classification given prompts)"
  },
  {
    "objectID": "project/dsp/2/index.html#presentation",
    "href": "project/dsp/2/index.html#presentation",
    "title": "LLM text classification Shiny App (R)",
    "section": "Presentation",
    "text": "Presentation"
  },
  {
    "objectID": "project/dsp/2/index.html#project-overview",
    "href": "project/dsp/2/index.html#project-overview",
    "title": "LLM text classification Shiny App (R)",
    "section": "Project Overview",
    "text": "Project Overview\nThe shiny app is designed to provide visualization for the health data science summer fellowship projects, supervised by Dr. Alyssa Bilinski. You could check the details of project from the presentation slides.\nDirect Link to App"
  },
  {
    "objectID": "publication/pub2/index.html",
    "href": "publication/pub2/index.html",
    "title": "GJR-GARCH Volatility Modeling under NIG and ANN for Predicting Top Cryptocurrencies",
    "section": "",
    "text": "Cryptocurrencies are currently traded worldwide, with hundreds of different currencies in existence and even more on the way. This study implements some statistical and machine learning approaches for cryptocurrency investments. First, we implement GJR-GARCH over the GARCH model to estimate the volatility of ten popular cryptocurrencies based on market capitalization: Bitcoin, Bitcoin Cash, Bitcoin SV, Chainlink, EOS, Ethereum, Litecoin, TETHER, Tezos, and XRP. Then, we use Monte Carlo simulations to generate the conditional variance of the cryptocurrencies using the GJR-GARCH model, and calculate the value at risk (VaR) of the simulations. We also estimate the tail-risk using VaR backtesting. Finally, we use an artificial neural network (ANN) for predicting the prices of the ten cryptocurrencies. The graphical analysis and mean square errors (MSEs) from the ANN models confirmed that the predicted prices are close to the market prices. For some cryptocurrencies, the ANN models perform better than traditional ARIMA models.\n\n\n    \n        \n    \n    \n        \n    \n\n\nShare on\n\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@article{mostafa2021,\n  author = {Mostafa, Fahad and Saha, Pritam and Rafiqul Islam, Mohammad\n    and Nguyen, Nguyet},\n  title = {GJR-GARCH {Volatility} {Modeling} Under {NIG} and {ANN} for\n    {Predicting} {Top} {Cryptocurrencies}},\n  journal = {Journal of Risk and Financial Management},\n  date = {2021-09-03},\n  url = {https://kaizhongmu.github.io/publication/pub2/},\n  doi = {10.3390/jrfm14090421},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMostafa, Fahad, Pritam Saha, Mohammad Rafiqul Islam, and Nguyet Nguyen.\n2021. “GJR-GARCH Volatility Modeling Under NIG and ANN for\nPredicting Top Cryptocurrencies.” Journal of Risk and\nFinancial Management, September. https://doi.org/10.3390/jrfm14090421."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "research.html#research-interests",
    "href": "research.html#research-interests",
    "title": "",
    "section": "Research Interests",
    "text": "Research Interests\n\n\n\n\n\n\n\n\n\n\nAI + Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLongitudinal Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeuroimaging & Network Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian statistics\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research.html#publications-manuscripts",
    "href": "research.html#publications-manuscripts",
    "title": "",
    "section": "Publications & Manuscripts",
    "text": "Publications & Manuscripts\n\nA Reliable Change Estimation Method for Severely Cognitively Impaired Populations\nAuthors: Mu, K., Zhang, D., & Eloyan, A.\nStatus: Manuscript in preparation for submission to Alzheimer’s & Dementia\nRole: First and corresponding author; draft completed July 2025, currently undergoing formatting.\n\n\nLongitudinal miRNA Profiles in Breast Cancer Tissue and Plasma: Associations with Hormone Receptors, Response, and Survival\nAuthors: Ryspayeva, D., Seyhan, A. A., Mu, K., Liu, M., Purcell, C., MacDonald, W. J., Halytskiy, V., Drevytska, T., Inomistova, M., Khranovska, N., Potorocha, O., Taran, L., Sumkina, O., Smolanka, I. Sr., & El-Deiry, W. S.\nJournal: World Journal of Clinical Oncology\nStatus: Manuscript submitted October 2025 (Manuscript No. 115287)\nRole: Third author."
  },
  {
    "objectID": "research.html#current-past-research",
    "href": "research.html#current-past-research",
    "title": "",
    "section": "Current & Past Research",
    "text": "Current & Past Research\n\nAn Bayesian-based RNN Method for MNAR/MAR + left-censored longitudinal\nAdvisor: Dr. Ani Eloyan\nDuration: 09/25 – Present\n\nThis thesis extends prior SRB-based work by replacing its predefined linear trend and single-retest constraint with an RNN architecture to fully utilize multiple follow-ups while preserving Bayesian uncertainty quantification.\nConstructed a unified likelihood that simultaneously handles MNAR/MAR missingness and left-censoring in Alzheimer’s longitudinal data through RNN hidden states that capture temporal dependence.\nImplemented Bayesian inference, specifically variational inference, enabling the RNN to inherently model incomplete data while maintaining uncertainty quantification and computational efficiency.\nBasic derivations completed, with manuscript preparation in progress; to be presented as a poster at Brown in April 2026\n\n\n\nA Bayesian Cognitive Change Method for Censored Longitudinal Data\nAdvisor: Dr. Ani Eloyan\nDuration: 01/25 – 07/25\n\nDesigned simulation study and an R Shiny App to show that SRB method underestimates cognitive change in Severe dementia whose follow-ups are left-censored at floor of NACC test battery in Longitudinal EOAD Study (LEADs)\nExtended the SRB method by introducing Bayesian Inference that models censored follow-ups as random variables under a censored (Tobit-type) likelihood, realized through the MCMC (Hastings-Metropolis) implemented in R Stan.\nConducted simulation and real-data analyses demonstrating the proposed method reduce the underestimation while preserving the interpretability aspect of the SRB index and providing credible intervals for uncertainty quantification.\nResulted in a manuscript under formatting and presentations at three conferences.\n\n\n\nNonlinear Mixed Effect Model for Alzheimer’s Disease Biomarker\nAdvisor: Dr. Ani Eloyan\nDuration: 09/24 – 12/24\n\nConducted literature reviews on vivo Alzheimer’s disease imaging biomarker (tau-PET, WMH in MRI) and Spline modeling techniques (B-spline, Natural-spline, penalized-spline)\nConstructed a P-spline design matrix based on cubic B-spline basis function within mixed-effects framework to model the nonlinear increment of White Matter Hyperintensities (WMH) volume in longitudinal MRI data.\n\n\n\nTranslational Medical Research Collaboration\nCollaborator: Dr. Dinara Ryspayeva, Dr. Attila Seyhan, Dr. Wafik El-Deiry. Legorreta Cancer Center\nDuration: 03/25 – 09/25\n\nLed and conducted a full pipeline of statistical analyses for breast cancer miRNA biomarker discovery —including missing data imputation, survival data collection guidance, survival analysis.\nIdentified MAR assumption via sensitivity analyses and applied conditional mean imputation for missing data\nBuilt Cox regression model to evaluate the predictive associations of miR-34a, miR-137, miR-373, miR-124a, and miR-155 with overall survival, taking account for both biological interpretability (up/down regulation, interactions) and statistical rigor (PH assumption, multicollinearity, influential point).\nResulted in a manuscript currently under submission to “World Journal of Clinical Oncology”\n\n\n\nHealth Data Science Summer Fellowship Program\nAdvisor: Dr. Alyssa Bilinski\nDuration: 06/25 – 08/25\n\nReceived training in data tidying in R, visualization in Tableau, data management in SQL, and causal inference.\nApplied Large Language Model (LLMs) (OpenAI) for free-text classification of public response to private equity, aiming to evaluate and improve prompt effectiveness in domain-specific text understanding.\nBuilt an automated evaluation pipeline and interactive Shiny App to benchmark AI annotations against human ground truth.\nResulted in oral presentation to program faculty and fellows at the final symposium.\n\n\n\nComparative Analysis of Machine Learning Models for Unemployment Prediction\nAdvisor: Dr. Colin Cameron\nDuration: 12/22 – 03/23\n\nCollected and managed unemployment related data through APIs, web scraping (BeautifulSoup) in Python\nImplemented and Compared multiple regression models, including OLS, LASSO, and a novel dimension-reduced local weighted regression applying PCA, for unemployment rate prediction.\nEvaluated and visualized model results in Stata and RStudio, using key metrics like Mean Squared Error (MSE), Mean Absolute Deviation (MAD), and R-squared, accuracy, and efficiency.\n\n\n\nGraph-Based Change-Point Detection Method for High-Dimensional Network Data\nAdvisor: Dr. Hao Chen, Assoc. Prof. of Statistics, UCD\nDuration: 09/22 – 01/23\n\nConducted literature reviews on graph theory and graph models (Stochastic Block Models, Erdős–Rényi graphs and Configuration Model)\nRefined gSeg R-package, implementing a combination of Image Analysis and SCAN statistics test in detecting change-point in dynamic social networks, while optimizing the algorithms for computational efficiency and scalability\nDesigned and conducted simulation studies in R to evaluate the predictive performance of the proposed method against conventional approaches, across different levels of graph sparsity, edge density, and network connectivity"
  },
  {
    "objectID": "research.html#presentations-and-conferences",
    "href": "research.html#presentations-and-conferences",
    "title": "",
    "section": "Presentations and Conferences",
    "text": "Presentations and Conferences\n\n\n\n\n\n\n\n\nDate\nConference\nTalk & Award\n\n\n\n\nJun 9-11, 2025\nSouthern Regional Council On Statistics (SRCOS)\nPoster presentation & Boyd Harshburger Travel Award Winner\n\n\nAug 14-16, 2025\nStatistics in Pharmaceuticals (SIP)\nPoster presentation\n\n\nOct 10, 2025\nASA NJ Statistics Workshop 2025\nPoster presentation"
  },
  {
    "objectID": "research/longitudinal/index.html",
    "href": "research/longitudinal/index.html",
    "title": "Longitudinal Data",
    "section": "",
    "text": "MNAR Missing Data, Censored Data, Nonlinearity, survival analysis, Alzheimer’s Disease"
  },
  {
    "objectID": "research/longitudinal/index.html#interest-keywords",
    "href": "research/longitudinal/index.html#interest-keywords",
    "title": "Longitudinal Data",
    "section": "",
    "text": "MNAR Missing Data, Censored Data, Nonlinearity, survival analysis, Alzheimer’s Disease"
  },
  {
    "objectID": "research/longitudinal/index.html#related-experience",
    "href": "research/longitudinal/index.html#related-experience",
    "title": "Longitudinal Data",
    "section": "Related Experience",
    "text": "Related Experience\n\nResearch Project: A Bayesian Cognitive Change Method for Censored Longitudinal Data\nResearch Project: An Bayesian-based RNN Method for MNAR/MAR + left-censored longitudinal\nResearch Project: Nonlinear Mixed Effect model for Alzheimer’s Disease Biomarker\nResearch project: Comparative Analysis of ML Models for Unemployment Prediction\nCollaboration: Translational Medical Research collaboration\nManuscript: A Reliable Change Estimation Method for Severely Cognitively Impaired Populations\nManuscript: Longitudinal miRNA Profiles in Breast Cancer Tissue and Plasma: Associations with Hormone Receptors, Response, and Survival."
  },
  {
    "objectID": "research/longitudinal/index.html#related-story",
    "href": "research/longitudinal/index.html#related-story",
    "title": "Longitudinal Data",
    "section": "Related Story",
    "text": "Related Story\n\n2021: Undergraduate time-series coursework sparked my interest in time-dependent data, particularly in estimation under temporal dependence.\n2022: Dr. Colin Cameron mentored my first independent research on applying machine learning to time-series prediction. Beyond technical skills, his rigor and passion for research deeply shaped my academic mindset.\n2024-2025: Dr. Ani Eloyan introduced me to biostatistical research in Alzheimer’s disease and guided me into longitudinal data analysis. I am grateful for her trust and support in allowing me to lead two projects, through which I developed Bayesian integration methods for handling censored and missing data and further extended a Bayesian-based RNN to simultaneously address both challenges in EOAD longitudinal datasets."
  },
  {
    "objectID": "research/MNAR/index.html",
    "href": "research/MNAR/index.html",
    "title": "Bayesian statistics",
    "section": "",
    "text": "I’m drawn to the Bayesian approach to thinking about uncertainty, so its application to biomedical data is of interest to me. I am interested in the MCMC and Variational Inference for large data sets, the use of Bayesian methods in deep learning, the semi-parametric Bayesian. I have an ongoing project integrating Bayesian with RNN in simultaneously handles MNAR/MAR missingness and left-censoring in Alzheimer’s longitudinal data. I have a post project extending a cognitive measurement method under the Bayesian framework to handle left-censored data."
  },
  {
    "objectID": "research/MNAR/index.html#interest-keywords",
    "href": "research/MNAR/index.html#interest-keywords",
    "title": "Bayesian statistics",
    "section": "",
    "text": "I’m drawn to the Bayesian approach to thinking about uncertainty, so its application to biomedical data is of interest to me. I am interested in the MCMC and Variational Inference for large data sets, the use of Bayesian methods in deep learning, the semi-parametric Bayesian. I have an ongoing project integrating Bayesian with RNN in simultaneously handles MNAR/MAR missingness and left-censoring in Alzheimer’s longitudinal data. I have a post project extending a cognitive measurement method under the Bayesian framework to handle left-censored data."
  },
  {
    "objectID": "research/MNAR/index.html#related-experience",
    "href": "research/MNAR/index.html#related-experience",
    "title": "Bayesian statistics",
    "section": "Related Experience",
    "text": "Related Experience\n\nResearch Project: A Bayesian Cognitive Change Method for Censored Longitudinal Data\nResearch Project: An Bayesian-based RNN Method for MNAR/MAR + left-censored\nManuscript: A Reliable Change Estimation Method for Severely Cognitively Impaired Populations"
  },
  {
    "objectID": "research/MNAR/index.html#related-story",
    "href": "research/MNAR/index.html#related-story",
    "title": "Bayesian statistics",
    "section": "Related Story",
    "text": "Related Story\nThe story of Bayesian statistics and me began with a research project where I encountered a non-identifiability issue when using MLE to fit a mixed-effects model. Learning that Bayesian inference might offer a solution, I enrolled in a Bayesian course and received systematic training—from explicit likelihood to MCMC methods for non-explicit likelihoods. As the course concluded, another project of mine entered the stage of method development. The Bayesian framework inspired me to address left-censored data with a simpler mathematical formulation while providing uncertainty quantification for clinicians. Building on this foundation, I am now conducting Bayesian inference under an RNN architecture to handle MNAR and left-censored data simultaneously."
  },
  {
    "objectID": "portfolio/dsp/nlp-sales/index.html",
    "href": "portfolio/dsp/nlp-sales/index.html",
    "title": "Predicting Product Success Using Customer Reviews and Sales Data",
    "section": "",
    "text": "Notebook GitHub (Private until finished)\n\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{islam2024,\n  author = {Islam, Rafiq},\n  title = {Predicting {Product} {Success} {Using} {Customer} {Reviews}\n    and {Sales} {Data}},\n  date = {2024-10-11},\n  url = {https://kaizhongmu.github.io/portfolio/dsp/nlp-sales/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nIslam, Rafiq. 2024. “Predicting Product Success Using Customer\nReviews and Sales Data.” October 11, 2024. https://kaizhongmu.github.io/portfolio/dsp/nlp-sales/."
  },
  {
    "objectID": "portfolio/dsp/dp-nlp/index.html",
    "href": "portfolio/dsp/dp-nlp/index.html",
    "title": "Disease diagnosis using classification and NLP",
    "section": "",
    "text": "Team Members\nRebecca Ceppas de Castro, Fulya Tastan, Philip Barron, Mohammad Rafiqul Islam, Nina Adhikari, Viraj Meruliya\nAutomatic Symptom Detection (ASD) and Automatic Diagnosis (AD) have seen several advances in recent years. Patients and medical professionals would benefit from tools that can aid in diagnosing diseases based on antecedents and presenting symptoms. The lack of quality healthcare in many parts of the world makes solving this problem a matter of utmost urgency. The aim of this project is to build a tool that can diagnose a disease based on a list of symptoms and contribute to our understanding of automatic diagnosis.\nProject Details\nSlides\nExecutive Summary\nGitHub Repo\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{ceppas_de_castro,_fulya_tastan,_philip_barron,_mohammad_rafiqul_islam,_nina_adhikari,_viraj_meruliya_2024,\n  author = {Ceppas de Castro, Fulya Tastan, Philip Barron, Mohammad\n    Rafiqul Islam, Nina Adhikari, Viraj Meruliya , Rebecca},\n  title = {Disease Diagnosis Using Classification and {NLP}},\n  date = {2024-06-18},\n  url = {https://kaizhongmu.github.io/portfolio/dsp/dp-nlp/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCeppas de Castro, Fulya Tastan, Philip Barron, Mohammad Rafiqul Islam,\nNina Adhikari, Viraj Meruliya, Rebecca. 2024. “Disease Diagnosis\nUsing Classification and NLP.” June 18, 2024. https://kaizhongmu.github.io/portfolio/dsp/dp-nlp/."
  },
  {
    "objectID": "portfolio/dsp/ecommerce/index.html",
    "href": "portfolio/dsp/ecommerce/index.html",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "",
    "text": "Notebook"
  },
  {
    "objectID": "portfolio/dsp/ecommerce/index.html#project-overview",
    "href": "portfolio/dsp/ecommerce/index.html#project-overview",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "Project Overview",
    "text": "Project Overview\n\nThis is a preliminary level linear regression based machine learning project to investigate the feature importance for an e-commerce based company or simply building a predictive model to generate insights on different features."
  },
  {
    "objectID": "portfolio/dsp/ecommerce/index.html#dataset",
    "href": "portfolio/dsp/ecommerce/index.html#dataset",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "Dataset",
    "text": "Dataset\nThe data is collected from kaggle.com. It contains 500 observations with the following columns\n\nEmail: Email address of the customers\nAddress: Physical mailing address of the customers\n\nAvatar: The fancy avater of the customers\n\nAvg. Session Length: Average session lenght spent either on app or web\n\nLength of Membership: Length of the membership of the customers with the e-commerce company\n\nTime on App: Time spent on the mobile app\n\nTime on Website: Time spent on web based browser\n\nYearly Amount Spent: This is the dependent variable."
  },
  {
    "objectID": "portfolio/dsp/ecommerce/index.html#stakeholders",
    "href": "portfolio/dsp/ecommerce/index.html#stakeholders",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "Stakeholders",
    "text": "Stakeholders\nIf the company wants to decide whether to focus their efforts on the mobile app or the website."
  },
  {
    "objectID": "portfolio/dsp/ecommerce/index.html#key-performance-indicators-kpis",
    "href": "portfolio/dsp/ecommerce/index.html#key-performance-indicators-kpis",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "Key Performance Indicators (KPIs)",
    "text": "Key Performance Indicators (KPIs)\n\nAll the quantitative features were considered to find their importance on the Yearly Amount Spent variable. However, it was found that Length of Membership, Time on App, Avg. Session Length have the highest impact on the dependent variable in decreasing order."
  },
  {
    "objectID": "portfolio/dsp/ecommerce/index.html#modeling",
    "href": "portfolio/dsp/ecommerce/index.html#modeling",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "Modeling",
    "text": "Modeling\n\\[\\begin{align*}\n\\text{Yearly Amount Spent}&=-1054.215476+25.362665\\times (\\text{Avg. Session Length})\\\\\n& +38.823679\\times (\\text{Time on App})+0.803568\\times (\\text{Time on Website})\\\\\n& + 61.549053\\times (\\text{Length of Membership})\n\\end{align*}\\]"
  },
  {
    "objectID": "portfolio/dsp/ecommerce/index.html#results-and-outcome",
    "href": "portfolio/dsp/ecommerce/index.html#results-and-outcome",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "Results and Outcome",
    "text": "Results and Outcome\n\nModel Explanation\nBased on the model above, we can sumerize as follows\n\nIf everything else remain unchanged, a 1 unit increase in Avg. Session Length is associated with an increase of \\(25.36\\) in total Yearly Amount Spent\n\nIf everything else remain unchanged, a 1 unit increase in Time on App is associated with an increase of \\(38.82\\) in total Yearly Amount Spent\n\nIf everything else remain unchanged, a 1 unit increase in Time on Website is associated with an increase of \\(0.80\\) in total Yearly Amount Spent\n\nIf everything else remain unchanged, a 1 unit increase in Length of Membership is associated with an increase of \\(61.55\\) in total Yearly Amount Spent\n\nNow the key question, should the company focus more on Time on App more?\n\nThe answer to the question above is a little bit tricky. Based on the modeling approach, appearantly it may seems that time on app has more impact than the time on web. However, the most significant factor seems the Length of Memberhsip. So we need further analysis of this two features to properly answer if the company should focus more on app.\n\n\n\nModel Accuracy\nThe model above returns a MAE of 7.99, MSE of 102.72, RMSE of 10.14, and \\(R^2=98.46\\%\\)"
  },
  {
    "objectID": "portfolio/dsp/autoloan/index.html",
    "href": "portfolio/dsp/autoloan/index.html",
    "title": "Auto Loan Decision Model",
    "section": "",
    "text": "Report Presentation"
  },
  {
    "objectID": "portfolio/dsp/autoloan/index.html#objective",
    "href": "portfolio/dsp/autoloan/index.html#objective",
    "title": "Auto Loan Decision Model",
    "section": "Objective",
    "text": "Objective\n\nThe Auto Loan Credit Decisioning Model project aimed to enhance the application decision process for auto loans by leveraging machine learning techniques to classify applicants into approved or rejected categories. The project focused on improving prediction accuracy and ensuring fairness across demographic groups while addressing challenges like class imbalance and missing data."
  },
  {
    "objectID": "portfolio/dsp/autoloan/index.html#data-overview",
    "href": "portfolio/dsp/autoloan/index.html#data-overview",
    "title": "Auto Loan Decision Model",
    "section": "Data Overview",
    "text": "Data Overview\n\nDataset: Auto loan account data with 21,000 training records and 5,400 test records.\nFeatures: 43 columns, including borrower creditworthiness, loan application attributes, and demographics.\nTarget Variable: ‘Bad Flag’ (binary), indicating ‘Poor Credit Quality’ (95.5%) or ‘Good Credit Quality’ (4.5%).\nChallenges: Significant class imbalance and high proportion of missing values in several predictors."
  },
  {
    "objectID": "portfolio/dsp/autoloan/index.html#methodology",
    "href": "portfolio/dsp/autoloan/index.html#methodology",
    "title": "Auto Loan Decision Model",
    "section": "Methodology",
    "text": "Methodology\n\nExploratory Data Analysis (EDA):\n\nInvestigated class distributions and correlations with the target variable.\nAddressed missing values using mode and median imputations based on feature types.\nAnalyzed key predictors such as FICO scores, loan-to-value ratios, and credit utilization rates.\n\nModel Development:\n\nBuilt models using Logistic Regression, Decision Tree, and Random Forest classifiers.\nConducted hyperparameter tuning via GridSearchCV for optimal model settings.\nAddressed class imbalance with resampling techniques like SMOTE.\n\nEvaluation Metrics:\n\nPrioritized ROC-AUC, F1-Score, and classification reports over accuracy to account for imbalanced data."
  },
  {
    "objectID": "portfolio/dsp/autoloan/index.html#results",
    "href": "portfolio/dsp/autoloan/index.html#results",
    "title": "Auto Loan Decision Model",
    "section": "Results",
    "text": "Results\n\nBest Model: Random Forest Classifier with an ROC-AUC score of 0.8078.\nPerformance:\n\nTest data accuracy: 94.08%\nPrecision (Class 0): 96.16%\nRecall (Class 0): 97.70%\n\nFairness Analysis:\n\nGender-neutral approval rates.\nNo significant racial bias in decisions."
  },
  {
    "objectID": "portfolio/dsp/autoloan/index.html#key-insights",
    "href": "portfolio/dsp/autoloan/index.html#key-insights",
    "title": "Auto Loan Decision Model",
    "section": "Key Insights",
    "text": "Key Insights\n\nFICO scores, loan-to-value ratios, and credit utilization rates were strong predictors of credit quality.\nHigher class imbalance and overfitting issues were observed with resampling techniques."
  },
  {
    "objectID": "portfolio/dsp/autoloan/index.html#innovations",
    "href": "portfolio/dsp/autoloan/index.html#innovations",
    "title": "Auto Loan Decision Model",
    "section": "Innovations",
    "text": "Innovations\n\nImplemented Local Interpretable Model-agnostic Explanations (LIME) for model transparency, allowing stakeholders to understand prediction outcomes."
  },
  {
    "objectID": "portfolio/dsp/autoloan/index.html#conclusion",
    "href": "portfolio/dsp/autoloan/index.html#conclusion",
    "title": "Auto Loan Decision Model",
    "section": "Conclusion",
    "text": "Conclusion\nThe Random Forest Classifier demonstrated strong predictive performance and fairness, providing a reliable foundation for auto loan decisioning. Opportunities for further improvements include advanced resampling methods, enhanced feature engineering, and exploring models like XGBoost or LightGBM for better results.\nKeywords: Auto Loan, Predictive Modeling, Random Forest, Class Imbalance, Machine Learning, LIME."
  },
  {
    "objectID": "portfolio/spd/holmc/index.html",
    "href": "portfolio/spd/holmc/index.html",
    "title": "Python Package: holmc",
    "section": "",
    "text": "A Higher-Order Langevin Monte Carlo Sampler for Bayesian Learning\nHolmc provides third- and fourth-order Langevin MCMC samplers designed for efficient sampling in Bayesian regression and classification problems. The detail of the package can be found here\nShare on\n\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{islam2025,\n  author = {Islam, Rafiq},\n  title = {Python {Package:} Holmc},\n  date = {2025-08-14},\n  url = {https://kaizhongmu.github.io/portfolio/spd/holmc/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nIslam, Rafiq. 2025. “Python Package: Holmc.” August 14,\n2025. https://kaizhongmu.github.io/portfolio/spd/holmc/."
  },
  {
    "objectID": "teaching/sp24.html",
    "href": "teaching/sp24.html",
    "title": "Spring 2024: MAP4170 Introduction to Actuarial Mathematics",
    "section": "",
    "text": "One of the course objectives is for each student to develop a mastery of financial mathematics used by actuaries, based on the mathematics of interest theory. Other course objectives are for each student to understand the long-term individual study commitment necessary to achieve a designation within one of the actuarial societies and for each student to increase their knowledge of the actuarial profession\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "teaching/fall21.html",
    "href": "teaching/fall21.html",
    "title": "Fall 2021 and Spring 2022: PreCalculus and Algebra",
    "section": "",
    "text": "As a lecture TA, my job was to facilitate the instructor during the class. This included helping students in class activities such as answering short questions that counted as class attendance, checking students’ eligibility forms for taking this course, and others as needed by the instructor.  As a lab TA I worked in a computer lab where students take their weekly quizzes and midterm tests.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "teaching/fall23.html",
    "href": "teaching/fall23.html",
    "title": "Fall 2023: MAP4170 Introduction to Actuarial Mathematics",
    "section": "",
    "text": "Worked as a greader for this course.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "teaching/fall22.html",
    "href": "teaching/fall22.html",
    "title": "Fall 2022: MAC2311 Calculus and Analytic Geometry I",
    "section": "",
    "text": "Students who have substantial knowledge of precalculus and algebra may require to take this course as a mathematics requirement depending on their majors. The topic of this course includes but is not limited to Foundation for calculus: Functions and Limits, Derivative, The Definite Integral, and Constructing Antiderivatives.  As a recitation instructor for this course, I ran two poster presentation sessions of 30 students in each group where they presented mathematical problems and their solutions step by step to their peer classmates followed by a group activity where they solved another set of problems. I also graded their exam scripts and weekly posters.\n\n\n\n\n\n\n Back to top"
  }
]